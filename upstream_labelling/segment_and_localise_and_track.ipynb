{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2e295e0",
   "metadata": {},
   "source": [
    "# Segment, localise and track\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "988f1019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA RTX A6000\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cellpose.core:** TORCH CUDA version installed and working. **\n",
      "INFO:cellpose.core:>>>> using GPU\n",
      "INFO:cellpose.models:>> cyto3 << model set to be used\n",
      "INFO:cellpose.models:>>>> model diam_mean =  30.000 (ROIs rescaled to this size during training)\n"
     ]
    }
   ],
   "source": [
    "from macrohet import dataio, tile, notify\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from cellpose import models\n",
    "import btrack \n",
    "import torch\n",
    "import os\n",
    "import dask.array as da\n",
    "import glob\n",
    "import zarr\n",
    "import logging\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "\n",
    "# # defining personal trained cellpose model to use\n",
    "# model_path = '/home/dayn/analysis/models/cellpose/PS0000/macrohet_seg'\n",
    "# model = models.CellposeModel(gpu=True, \n",
    "#                              pretrained_model=model_path)\n",
    "\n",
    "# ORRRR test the new cellpose model\n",
    "model = models.Cellpose(gpu=True, model_type='cyto3')\n",
    "\n",
    "# Initialize the logging configuration\n",
    "log_dir = \"logs\"  # Specify the directory where logs will be saved\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s]: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "# Add a FileHandler to save logs to a file in the specified directory\n",
    "log_file = os.path.join(log_dir, \"assay_processing.log\")\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter(\"%(asctime)s [%(levelname)s]: %(message)s\")\n",
    "file_handler.setFormatter(formatter)\n",
    "logging.getLogger().addHandler(file_handler)\n",
    "\n",
    "# Define a function to log progress and potential errors\n",
    "def log_progress(position, message):\n",
    "    logging.info(f\"Position {position}: {message}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76982865",
   "metadata": {},
   "source": [
    "### Define functions to tidy up main block of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65275f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define thresholds\n",
    "segment_size_thresh = 5000\n",
    "Mtb_load_thresh = 480\n",
    "\n",
    "# define tracking scale factor\n",
    "scale_factor = 1/5.04\n",
    "\n",
    "# define features to use for tracking \n",
    "features = [\n",
    "  \"area\",\n",
    "  \"major_axis_length\",\n",
    "  \"minor_axis_length\",\n",
    "  \"orientation\",\n",
    "  \"mean_intensity\",\n",
    "    ]\n",
    "\n",
    "# define tracker config fn to use, using a prob_not_assign = 0.1\n",
    "config_fn = '/home/dayn/analysis/models/btrack/particle_config_pnassign.json'\n",
    "# define tracker config fn to use\n",
    "# config_fn = '/home/dayn/analysis/btrack/models/particle_config.json'\n",
    "\n",
    "def segment(frame, model = model, channels = [0,0], diameter = 350,#325\n",
    "            min_size = 5000 #2500\n",
    "           ):\n",
    "    \n",
    "#     masks, flows, styles, diams = model.eval(frame, # for default model\n",
    "#                                              channels = channels, \n",
    "#                                              diameter = diameter, \n",
    "#                                              min_size = min_size, \n",
    "#                                              )\n",
    "    masks, flows, styles = model.eval(frame, # for personal model\n",
    "                                      channels = channels, \n",
    "                                      diameter = diameter, \n",
    "                                      min_size = min_size, \n",
    "                                      )\n",
    "    return masks\n",
    "\n",
    "\n",
    "def localise(masks, intensity_image, properties=tuple(features), use_weighted_centroid = False):\n",
    "    \n",
    "    # localise objs in images\n",
    "    objects = btrack.utils.segmentation_to_objects(segmentation=masks,\n",
    "                                                   intensity_image=intensity_image, \n",
    "                                                   properties=properties,\n",
    "                                                   scale=(scale_factor,scale_factor),\n",
    "                                                   use_weighted_centroid=use_weighted_centroid, \n",
    "                                                   )\n",
    "                                                   \n",
    "    return objects\n",
    "\n",
    "\n",
    "def track(objects, masks, config_fn, search_radius = 20):\n",
    "\n",
    "    # initialise a tracker session using a context manager\n",
    "    with btrack.BayesianTracker() as tracker:\n",
    "        # configure the tracker using a config file\n",
    "        tracker.configure(config_fn)\n",
    "        # set max search radius\n",
    "        tracker.max_search_radius = search_radius\n",
    "        # define tracking method\n",
    "        tracker.tracking_updates = [\"MOTION\", \"VISUAL\"]\n",
    "        # redefine features so that both channels are included in track measurements\n",
    "        tracker.features = list(objects[0].properties.keys())\n",
    "        # append the objects to be tracked\n",
    "        tracker.append(objects)\n",
    "        # set the tracking volume\n",
    "        tracker.volume=((0, masks.shape[-2]*scale_factor), (0, masks.shape[-1]*scale_factor))\n",
    "        # track them (in interactive mode)\n",
    "        tracker.track(step_size=25)\n",
    "        # generate hypotheses and run the global optimizer\n",
    "        tracker.optimize()\n",
    "        # store the tracks\n",
    "        tracks = tracker.tracks\n",
    "\n",
    "    return tracks\n",
    "\n",
    "\n",
    "def otsu_threshold_stack(images):\n",
    "    \"\"\"\n",
    "    Function to characterise intra-Mφ Mtb load\n",
    "    Computes Otsu's threshold value and returns a binary segmentation for\n",
    "    each image in a time series of grayscale images.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    images : ndarray\n",
    "        A 3D array of shape (n_images, height, width) containing a time series\n",
    "        of grayscale images.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    ndarray\n",
    "        A boolean array of shape (n_images, height, width) containing the\n",
    "        binary segmentation for each image in the time series.\n",
    "    \"\"\"\n",
    "    segmentations = np.zeros(images.shape, dtype=bool)\n",
    "    for i, image in tqdm(enumerate(images), \n",
    "                         total=len(images), \n",
    "                         leave=False, \n",
    "                         desc='Otsu segmenting'):\n",
    "        loaded_image = image.compute().compute()\n",
    "        threshold = threshold_otsu(loaded_image)\n",
    "        segmentations[i] = loaded_image > threshold\n",
    "        \n",
    "    return segmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3751ef77",
   "metadata": {},
   "source": [
    "### Load experiment of choice\n",
    "\n",
    "The Opera Phenix is a high-throughput confocal microscope that acquires very large 5-dimensional (TCZXY) images over several fields of view in any one experiment. Therefore, a lazy-loading approach is chosen to mosaic, view and annotate these images. This approach depends upon Dask and DaskFusion. The first step is to load the main metadata file (typically called `Index.idx.xml` and located in the main `Images` directory) that contains the image filenames and associated TCXZY information used to organise the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d20db67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading metadata XML file...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b29f7fb169a42a4afa3206be2a2e84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting metadata complete!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>State</th>\n",
       "      <th>URL</th>\n",
       "      <th>Row</th>\n",
       "      <th>Col</th>\n",
       "      <th>FieldID</th>\n",
       "      <th>PlaneID</th>\n",
       "      <th>TimepointID</th>\n",
       "      <th>ChannelID</th>\n",
       "      <th>FlimID</th>\n",
       "      <th>...</th>\n",
       "      <th>PositionZ</th>\n",
       "      <th>AbsPositionZ</th>\n",
       "      <th>MeasurementTimeOffset</th>\n",
       "      <th>AbsTime</th>\n",
       "      <th>MainExcitationWavelength</th>\n",
       "      <th>MainEmissionWavelength</th>\n",
       "      <th>ObjectiveMagnification</th>\n",
       "      <th>ObjectiveNA</th>\n",
       "      <th>ExposureTime</th>\n",
       "      <th>OrientationMatrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0103K1F1P1R1</td>\n",
       "      <td>Ok</td>\n",
       "      <td>r01c03f01p01-ch1sk1fk1fl1.tiff</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-2E-06</td>\n",
       "      <td>0.135466397</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-11-30T17:22:09.49+00:00</td>\n",
       "      <td>640</td>\n",
       "      <td>706</td>\n",
       "      <td>40</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>[[1.000989,0,0,10.0],[0,-1.000989,0,-6.8],[0,0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0103K1F1P1R2</td>\n",
       "      <td>Ok</td>\n",
       "      <td>r01c03f01p01-ch2sk1fk1fl1.tiff</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-2E-06</td>\n",
       "      <td>0.135466397</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-11-30T17:22:09.723+00:00</td>\n",
       "      <td>488</td>\n",
       "      <td>522</td>\n",
       "      <td>40</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[[1.000989,0,0,10.0],[0,-1.000989,0,-6.8],[0,0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0103K1F1P2R1</td>\n",
       "      <td>Ok</td>\n",
       "      <td>r01c03f01p02-ch1sk1fk1fl1.tiff</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135468394</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-11-30T17:22:10.067+00:00</td>\n",
       "      <td>640</td>\n",
       "      <td>706</td>\n",
       "      <td>40</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>[[1.000989,0,0,10.0],[0,-1.000989,0,-6.8],[0,0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0103K1F1P2R2</td>\n",
       "      <td>Ok</td>\n",
       "      <td>r01c03f01p02-ch2sk1fk1fl1.tiff</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135468394</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-11-30T17:22:10.287+00:00</td>\n",
       "      <td>488</td>\n",
       "      <td>522</td>\n",
       "      <td>40</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[[1.000989,0,0,10.0],[0,-1.000989,0,-6.8],[0,0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0103K1F1P3R1</td>\n",
       "      <td>Ok</td>\n",
       "      <td>r01c03f01p03-ch1sk1fk1fl1.tiff</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2E-06</td>\n",
       "      <td>0.135470405</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-11-30T17:22:10.627+00:00</td>\n",
       "      <td>640</td>\n",
       "      <td>706</td>\n",
       "      <td>40</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>[[1.000989,0,0,10.0],[0,-1.000989,0,-6.8],[0,0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388615</th>\n",
       "      <td>0612K150F9P1R2</td>\n",
       "      <td>Ok</td>\n",
       "      <td>r06c12f09p01-ch2sk150fk1fl1.tiff</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>149</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-2E-06</td>\n",
       "      <td>0.1351538</td>\n",
       "      <td>268191.66</td>\n",
       "      <td>2023-12-03T20:06:16.08+00:00</td>\n",
       "      <td>488</td>\n",
       "      <td>522</td>\n",
       "      <td>40</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[[1.000989,0,0,10.0],[0,-1.000989,0,-6.8],[0,0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388616</th>\n",
       "      <td>0612K150F9P2R1</td>\n",
       "      <td>Ok</td>\n",
       "      <td>r06c12f09p02-ch1sk150fk1fl1.tiff</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135155797</td>\n",
       "      <td>268191.66</td>\n",
       "      <td>2023-12-03T20:06:16.423+00:00</td>\n",
       "      <td>640</td>\n",
       "      <td>706</td>\n",
       "      <td>40</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>[[1.000989,0,0,10.0],[0,-1.000989,0,-6.8],[0,0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388617</th>\n",
       "      <td>0612K150F9P2R2</td>\n",
       "      <td>Ok</td>\n",
       "      <td>r06c12f09p02-ch2sk150fk1fl1.tiff</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>149</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135155797</td>\n",
       "      <td>268191.66</td>\n",
       "      <td>2023-12-03T20:06:16.657+00:00</td>\n",
       "      <td>488</td>\n",
       "      <td>522</td>\n",
       "      <td>40</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[[1.000989,0,0,10.0],[0,-1.000989,0,-6.8],[0,0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388618</th>\n",
       "      <td>0612K150F9P3R1</td>\n",
       "      <td>Ok</td>\n",
       "      <td>r06c12f09p03-ch1sk150fk1fl1.tiff</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2E-06</td>\n",
       "      <td>0.135157794</td>\n",
       "      <td>268191.66</td>\n",
       "      <td>2023-12-03T20:06:17+00:00</td>\n",
       "      <td>640</td>\n",
       "      <td>706</td>\n",
       "      <td>40</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>[[1.000989,0,0,10.0],[0,-1.000989,0,-6.8],[0,0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388619</th>\n",
       "      <td>0612K150F9P3R2</td>\n",
       "      <td>Ok</td>\n",
       "      <td>r06c12f09p03-ch2sk150fk1fl1.tiff</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>149</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2E-06</td>\n",
       "      <td>0.135157794</td>\n",
       "      <td>268191.66</td>\n",
       "      <td>2023-12-03T20:06:17.217+00:00</td>\n",
       "      <td>488</td>\n",
       "      <td>522</td>\n",
       "      <td>40</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[[1.000989,0,0,10.0],[0,-1.000989,0,-6.8],[0,0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>388620 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id State                               URL Row Col  \\\n",
       "0         0103K1F1P1R1    Ok    r01c03f01p01-ch1sk1fk1fl1.tiff   1   3   \n",
       "1         0103K1F1P1R2    Ok    r01c03f01p01-ch2sk1fk1fl1.tiff   1   3   \n",
       "2         0103K1F1P2R1    Ok    r01c03f01p02-ch1sk1fk1fl1.tiff   1   3   \n",
       "3         0103K1F1P2R2    Ok    r01c03f01p02-ch2sk1fk1fl1.tiff   1   3   \n",
       "4         0103K1F1P3R1    Ok    r01c03f01p03-ch1sk1fk1fl1.tiff   1   3   \n",
       "...                ...   ...                               ...  ..  ..   \n",
       "388615  0612K150F9P1R2    Ok  r06c12f09p01-ch2sk150fk1fl1.tiff   6  12   \n",
       "388616  0612K150F9P2R1    Ok  r06c12f09p02-ch1sk150fk1fl1.tiff   6  12   \n",
       "388617  0612K150F9P2R2    Ok  r06c12f09p02-ch2sk150fk1fl1.tiff   6  12   \n",
       "388618  0612K150F9P3R1    Ok  r06c12f09p03-ch1sk150fk1fl1.tiff   6  12   \n",
       "388619  0612K150F9P3R2    Ok  r06c12f09p03-ch2sk150fk1fl1.tiff   6  12   \n",
       "\n",
       "       FieldID PlaneID TimepointID ChannelID FlimID  ... PositionZ  \\\n",
       "0            1       1           0         1      1  ...    -2E-06   \n",
       "1            1       1           0         2      1  ...    -2E-06   \n",
       "2            1       2           0         1      1  ...         0   \n",
       "3            1       2           0         2      1  ...         0   \n",
       "4            1       3           0         1      1  ...     2E-06   \n",
       "...        ...     ...         ...       ...    ...  ...       ...   \n",
       "388615       9       1         149         2      1  ...    -2E-06   \n",
       "388616       9       2         149         1      1  ...         0   \n",
       "388617       9       2         149         2      1  ...         0   \n",
       "388618       9       3         149         1      1  ...     2E-06   \n",
       "388619       9       3         149         2      1  ...     2E-06   \n",
       "\n",
       "       AbsPositionZ MeasurementTimeOffset                        AbsTime  \\\n",
       "0       0.135466397                     0   2023-11-30T17:22:09.49+00:00   \n",
       "1       0.135466397                     0  2023-11-30T17:22:09.723+00:00   \n",
       "2       0.135468394                     0  2023-11-30T17:22:10.067+00:00   \n",
       "3       0.135468394                     0  2023-11-30T17:22:10.287+00:00   \n",
       "4       0.135470405                     0  2023-11-30T17:22:10.627+00:00   \n",
       "...             ...                   ...                            ...   \n",
       "388615    0.1351538             268191.66   2023-12-03T20:06:16.08+00:00   \n",
       "388616  0.135155797             268191.66  2023-12-03T20:06:16.423+00:00   \n",
       "388617  0.135155797             268191.66  2023-12-03T20:06:16.657+00:00   \n",
       "388618  0.135157794             268191.66      2023-12-03T20:06:17+00:00   \n",
       "388619  0.135157794             268191.66  2023-12-03T20:06:17.217+00:00   \n",
       "\n",
       "       MainExcitationWavelength MainEmissionWavelength ObjectiveMagnification  \\\n",
       "0                           640                    706                     40   \n",
       "1                           488                    522                     40   \n",
       "2                           640                    706                     40   \n",
       "3                           488                    522                     40   \n",
       "4                           640                    706                     40   \n",
       "...                         ...                    ...                    ...   \n",
       "388615                      488                    522                     40   \n",
       "388616                      640                    706                     40   \n",
       "388617                      488                    522                     40   \n",
       "388618                      640                    706                     40   \n",
       "388619                      488                    522                     40   \n",
       "\n",
       "       ObjectiveNA ExposureTime  \\\n",
       "0              1.1          0.2   \n",
       "1              1.1          0.1   \n",
       "2              1.1          0.2   \n",
       "3              1.1          0.1   \n",
       "4              1.1          0.2   \n",
       "...            ...          ...   \n",
       "388615         1.1          0.1   \n",
       "388616         1.1          0.2   \n",
       "388617         1.1          0.1   \n",
       "388618         1.1          0.2   \n",
       "388619         1.1          0.1   \n",
       "\n",
       "                                        OrientationMatrix  \n",
       "0       [[1.000989,0,0,10.0],[0,-1.000989,0,-6.8],[0,0...  \n",
       "1       [[1.000989,0,0,10.0],[0,-1.000989,0,-6.8],[0,0...  \n",
       "2       [[1.000989,0,0,10.0],[0,-1.000989,0,-6.8],[0,0...  \n",
       "3       [[1.000989,0,0,10.0],[0,-1.000989,0,-6.8],[0,0...  \n",
       "4       [[1.000989,0,0,10.0],[0,-1.000989,0,-6.8],[0,0...  \n",
       "...                                                   ...  \n",
       "388615  [[1.000989,0,0,10.0],[0,-1.000989,0,-6.8],[0,0...  \n",
       "388616  [[1.000989,0,0,10.0],[0,-1.000989,0,-6.8],[0,0...  \n",
       "388617  [[1.000989,0,0,10.0],[0,-1.000989,0,-6.8],[0,0...  \n",
       "388618  [[1.000989,0,0,10.0],[0,-1.000989,0,-6.8],[0,0...  \n",
       "388619  [[1.000989,0,0,10.0],[0,-1.000989,0,-6.8],[0,0...  \n",
       "\n",
       "[388620 rows x 35 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = '/mnt/SYNO/macrohet_syno/ND0002/'\n",
    "metadata_fn = os.path.join(base_dir, 'acquisition/Images/Index.idx.xml')\n",
    "metadata = dataio.read_harmony_metadata(metadata_fn)  \n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e739b419",
   "metadata": {},
   "source": [
    "### View assay layout and mask information (optional)\n",
    "\n",
    "The Opera Phenix acquires many time lapse series from a range of positions. The first step is to inspect the image metadata, presented in the form of an `Assaylayout/experiment_ID.xml` file, to show which positions correspond to which experimental assays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19c540d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading metadata XML file...\n",
      "Extracting metadata complete!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Strain</th>\n",
       "      <th>Compound</th>\n",
       "      <th>Concentration</th>\n",
       "      <th>ConcentrationEC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Row</th>\n",
       "      <th>Column</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">3</th>\n",
       "      <th>1</th>\n",
       "      <td>UNI</td>\n",
       "      <td>CTRL</td>\n",
       "      <td>0</td>\n",
       "      <td>EC0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNI</td>\n",
       "      <td>CTRL</td>\n",
       "      <td>0</td>\n",
       "      <td>EC0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WT</td>\n",
       "      <td>CTRL</td>\n",
       "      <td>0</td>\n",
       "      <td>EC0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WT</td>\n",
       "      <td>CTRL</td>\n",
       "      <td>0</td>\n",
       "      <td>EC0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>WT</td>\n",
       "      <td>PZA</td>\n",
       "      <td>60</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WT</td>\n",
       "      <td>PZA</td>\n",
       "      <td>60</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>WT</td>\n",
       "      <td>RIF</td>\n",
       "      <td>0.1</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WT</td>\n",
       "      <td>RIF</td>\n",
       "      <td>0.1</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WT</td>\n",
       "      <td>INH</td>\n",
       "      <td>0.04</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>WT</td>\n",
       "      <td>INH</td>\n",
       "      <td>0.04</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>WT</td>\n",
       "      <td>BDQ</td>\n",
       "      <td>0.02</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>WT</td>\n",
       "      <td>BDQ</td>\n",
       "      <td>0.02</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">4</th>\n",
       "      <th>3</th>\n",
       "      <td>WT</td>\n",
       "      <td>CTRL</td>\n",
       "      <td>0</td>\n",
       "      <td>EC0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WT</td>\n",
       "      <td>CTRL</td>\n",
       "      <td>0</td>\n",
       "      <td>EC0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>WT</td>\n",
       "      <td>PZA</td>\n",
       "      <td>400</td>\n",
       "      <td>EC99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WT</td>\n",
       "      <td>PZA</td>\n",
       "      <td>400</td>\n",
       "      <td>EC99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>WT</td>\n",
       "      <td>RIF</td>\n",
       "      <td>2</td>\n",
       "      <td>EC99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WT</td>\n",
       "      <td>RIF</td>\n",
       "      <td>2</td>\n",
       "      <td>EC99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WT</td>\n",
       "      <td>INH</td>\n",
       "      <td>2</td>\n",
       "      <td>EC99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>WT</td>\n",
       "      <td>INH</td>\n",
       "      <td>2</td>\n",
       "      <td>EC99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>WT</td>\n",
       "      <td>BDQ</td>\n",
       "      <td>2.5</td>\n",
       "      <td>EC99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>WT</td>\n",
       "      <td>BDQ</td>\n",
       "      <td>2.5</td>\n",
       "      <td>EC99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">5</th>\n",
       "      <th>3</th>\n",
       "      <td>RD1</td>\n",
       "      <td>CTRL</td>\n",
       "      <td>0</td>\n",
       "      <td>EC0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RD1</td>\n",
       "      <td>CTRL</td>\n",
       "      <td>0</td>\n",
       "      <td>EC0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RD1</td>\n",
       "      <td>PZA</td>\n",
       "      <td>60</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RD1</td>\n",
       "      <td>PZA</td>\n",
       "      <td>60</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RD1</td>\n",
       "      <td>RIF</td>\n",
       "      <td>0.1</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RD1</td>\n",
       "      <td>RIF</td>\n",
       "      <td>0.1</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RD1</td>\n",
       "      <td>INH</td>\n",
       "      <td>0.04</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RD1</td>\n",
       "      <td>INH</td>\n",
       "      <td>0.04</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RD1</td>\n",
       "      <td>BDQ</td>\n",
       "      <td>0.02</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RD1</td>\n",
       "      <td>BDQ</td>\n",
       "      <td>0.02</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">6</th>\n",
       "      <th>3</th>\n",
       "      <td>RD1</td>\n",
       "      <td>CTRL</td>\n",
       "      <td>0</td>\n",
       "      <td>EC0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RD1</td>\n",
       "      <td>CTRL</td>\n",
       "      <td>0</td>\n",
       "      <td>EC0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RD1</td>\n",
       "      <td>PZA</td>\n",
       "      <td>400</td>\n",
       "      <td>EC99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RD1</td>\n",
       "      <td>PZA</td>\n",
       "      <td>400</td>\n",
       "      <td>EC99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RD1</td>\n",
       "      <td>RIF</td>\n",
       "      <td>2</td>\n",
       "      <td>EC99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RD1</td>\n",
       "      <td>RIF</td>\n",
       "      <td>2</td>\n",
       "      <td>EC99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RD1</td>\n",
       "      <td>INH</td>\n",
       "      <td>2</td>\n",
       "      <td>EC99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RD1</td>\n",
       "      <td>INH</td>\n",
       "      <td>2</td>\n",
       "      <td>EC99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RD1</td>\n",
       "      <td>BDQ</td>\n",
       "      <td>2.5</td>\n",
       "      <td>EC99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RD1</td>\n",
       "      <td>BDQ</td>\n",
       "      <td>2.5</td>\n",
       "      <td>EC99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Strain Compound Concentration ConcentrationEC\n",
       "Row Column                                              \n",
       "3   1         UNI     CTRL             0             EC0\n",
       "    2         UNI     CTRL             0             EC0\n",
       "    3          WT     CTRL             0             EC0\n",
       "    4          WT     CTRL             0             EC0\n",
       "    5          WT      PZA            60            EC50\n",
       "    6          WT      PZA            60            EC50\n",
       "    7          WT      RIF           0.1            EC50\n",
       "    8          WT      RIF           0.1            EC50\n",
       "    9          WT      INH          0.04            EC50\n",
       "    10         WT      INH          0.04            EC50\n",
       "    11         WT      BDQ          0.02            EC50\n",
       "    12         WT      BDQ          0.02            EC50\n",
       "4   3          WT     CTRL             0             EC0\n",
       "    4          WT     CTRL             0             EC0\n",
       "    5          WT      PZA           400            EC99\n",
       "    6          WT      PZA           400            EC99\n",
       "    7          WT      RIF             2            EC99\n",
       "    8          WT      RIF             2            EC99\n",
       "    9          WT      INH             2            EC99\n",
       "    10         WT      INH             2            EC99\n",
       "    11         WT      BDQ           2.5            EC99\n",
       "    12         WT      BDQ           2.5            EC99\n",
       "5   3         RD1     CTRL             0             EC0\n",
       "    4         RD1     CTRL             0             EC0\n",
       "    5         RD1      PZA            60            EC50\n",
       "    6         RD1      PZA            60            EC50\n",
       "    7         RD1      RIF           0.1            EC50\n",
       "    8         RD1      RIF           0.1            EC50\n",
       "    9         RD1      INH          0.04            EC50\n",
       "    10        RD1      INH          0.04            EC50\n",
       "    11        RD1      BDQ          0.02            EC50\n",
       "    12        RD1      BDQ          0.02            EC50\n",
       "6   3         RD1     CTRL             0             EC0\n",
       "    4         RD1     CTRL             0             EC0\n",
       "    5         RD1      PZA           400            EC99\n",
       "    6         RD1      PZA           400            EC99\n",
       "    7         RD1      RIF             2            EC99\n",
       "    8         RD1      RIF             2            EC99\n",
       "    9         RD1      INH             2            EC99\n",
       "    10        RD1      INH             2            EC99\n",
       "    11        RD1      BDQ           2.5            EC99\n",
       "    12        RD1      BDQ           2.5            EC99"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_path = glob.glob(os.path.join(base_dir, 'acquisition/Assaylayout/*.xml'))[0]\n",
    "assay_layout = dataio.read_harmony_metadata(metadata_path, assay_layout=True,)# mask_exist=True,  image_dir = image_dir, image_metadata = metadata)\n",
    "assay_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a843967-19c1-4f33-9bf8-a4d331c520eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_processed_acq_IDs = [(3, 4), (4, 3), (4, 4), (3, 5), (3, 6), (3, 7), (3, 8), (3, 9),\n",
    "       (3, 10), (3, 11), (3, 12), (4, 5), (4, 6), (4, 7), (4, 8), (4, 9),\n",
    "       (4, 10), (3, 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45221981",
   "metadata": {},
   "source": [
    "# Segment, localise and track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "451353a1-ce0a-46d6-aef0-377f9c3a017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtb_channel = 0\n",
    "gfp_channel = 1\n",
    "manual_mtb_thresh_channel = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dedb723-dcbb-4985-8d0a-ab91c2be58ce",
   "metadata": {},
   "source": [
    "# Test if i have the capacity to stack these channels together like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7290f4e4-3031-418e-aa6d-ef532d0a2af6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4fd47aeff5b4a05af9ad08e44698b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress through positions:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Position (3, 1): Starting new acquisition\n"
     ]
    }
   ],
   "source": [
    "# Inside your loop, use the log_progress function to log progress and errors\n",
    "for (row, column), info in tqdm(assay_layout.iterrows(), desc='Progress through positions', total=len(assay_layout)):\n",
    "    try:\n",
    "        acq_ID = (row, column)\n",
    "        log_progress(acq_ID, \"Starting new acquisition\")\n",
    "        \n",
    "        # if info['Strain'] == 'UNI':\n",
    "        #     log_progress(acq_ID, \"Skipping uninfected acquisition for now\")\n",
    "        #     continue\n",
    "        # if acq_ID in already_processed_acq_IDs:\n",
    "        #     log_progress(acq_ID, \"Skipping already processed\")\n",
    "        #     continue\n",
    "        # if os.path.exists(os.path.join(base_dir, f'labels/macrohet_seg_model/{row, column}_first_pass_warea.h5')):\n",
    "        #     log_progress(acq_ID, \"Skipping already processed\")\n",
    "        #     continue\n",
    "\n",
    "        # process images using zarr\n",
    "        image_dir = os.path.join(base_dir, f'acquisition/zarr/{acq_ID}.zarr')\n",
    "        zarr_store = zarr.open(image_dir, mode='r')\n",
    "        images = zarr_store.images\n",
    "        # create a max projection\n",
    "        images = np.max(images, axis = 2)\n",
    "        \n",
    "        log_progress(acq_ID, \"Images loaded and stacked\")\n",
    "\n",
    "        # check if already segmented using m2 model\n",
    "    #     if os.path.exists(os.path.join(base_dir, f'labels/macrohet_seg_model/{row, column}.h5')):\n",
    "    #         continue\n",
    "    #     else:\n",
    "        log_progress(acq_ID, \"Starting segmentation\")\n",
    "\n",
    "    #     if os.path.exists(os.path.join(base_dir, f'labels/macrohet_seg_model/{row, column}_first_pass_seg_backup.h5')):\n",
    "    #         with btrack.io.HDF5FileHandler(os.path.join(base_dir, f'labels/macrohet_seg_model/{row, column}_first_pass_seg_backup.h5'), \n",
    "    #                                        'r', \n",
    "    #                                        obj_type='obj_type_1'\n",
    "    #                                        ) as reader:\n",
    "    # #             writer.write_objects(objects)\n",
    "    #             # writer.write_tracks(tracks)\n",
    "    #             masks = reader.segmentation\n",
    "    #         log_progress(acq_ID, \"Loaded previously calculated segmentation\")\n",
    "    #     else:\n",
    "        # segment images from gfp channel only\n",
    "        masks = np.stack([segment(frame) \n",
    "                          for frame in tqdm(images[:,gfp_channel,...],  # segmenting the GFP channel \n",
    "                                            desc = 'Segmenting')])\n",
    "\n",
    "        log_progress(acq_ID, \"Finished segmentation\")\n",
    "        \n",
    "        with btrack.io.HDF5FileHandler(os.path.join(base_dir, f'labels/cpv3/{row, column}_cpv3_mask_backup.h5'), \n",
    "                                           'w', \n",
    "                                           obj_type='obj_type_1'\n",
    "                                           ) as writer:\n",
    "    #             writer.write_objects(objects)\n",
    "                # writer.write_tracks(tracks)\n",
    "                writer.write_segmentation(masks)\n",
    "            \n",
    "        log_progress(acq_ID, \"Saved out masks\")  \n",
    "        \n",
    "        log_progress(acq_ID, \"Measuring Mtb area\")       \n",
    "        \n",
    "        # characterise Mtb growth using Otsu segmentation\n",
    "        # otsu_mtb = otsu_threshold(images[:,1,...]) # time consuming and non-deterministic when compared to hardcoded, could result in different thresholds for same image? \n",
    "        # characterise Mtb growth using hardcoded threshold :S\n",
    "        manual_mtb_thresh = np.where(images[:,mtb_channel,...] >= Mtb_load_thresh, True, False)\n",
    "        log_progress(acq_ID, \"Creating intensity image for localisation\")  \n",
    "        # reshape intensity image to be gfp, rfp on last axis for regionprops\n",
    "        intensity_image = np.stack([images[:,0,...], \n",
    "                                    images[:,1,...],  \n",
    "    #                                 otsu_mtb, \n",
    "                                    manual_mtb_thresh], axis = -1)\n",
    "        log_progress(acq_ID, \"Localising objects\")  \n",
    "        # localise objects\n",
    "        objects = localise(masks, \n",
    "                           intensity_image, \n",
    "                           )\n",
    "        log_progress(acq_ID, \"Filtering small objects\")  \n",
    "        # filter out small objects\n",
    "        objects = [o for o in objects if o.properties['area'] > segment_size_thresh]\n",
    "\n",
    "        log_progress(acq_ID, \"Adding infection labels to objects\")  \n",
    "        # add label for infection\n",
    "        for obj in objects:\n",
    "            obj.properties = ({\"Infected\": True} \n",
    "                                if obj.properties['mean_intensity'][manual_mtb_thresh_channel] > 0 # index 2 for manual mtb channel \n",
    "                                else {\"Infected\": False})\n",
    "            obj.properties = ({\"Mtb area px\": obj.properties['mean_intensity'][manual_mtb_thresh_channel]*obj.properties['area']})\n",
    "\n",
    "        with btrack.io.HDF5FileHandler(os.path.join(base_dir, f'labels/cpv3/{row, column}_cpv3_objects_backup.h5'), \n",
    "                                                   'w', \n",
    "                                                   obj_type='obj_type_1'\n",
    "                                                   ) as writer:\n",
    "                        writer.write_objects(objects)\n",
    "                        # writer.write_tracks(tracks)\n",
    "        \n",
    "        log_progress(acq_ID, \"Beginning tracking\")  \n",
    "        # track on upscaled config fn\n",
    "        tracks = track(objects, masks, config_fn, search_radius = 20)\n",
    "        log_progress(acq_ID, \"Saving tracking\")  \n",
    "        # save out \n",
    "        with btrack.io.HDF5FileHandler(os.path.join(base_dir, f'labels/cpv3/{row, column}_cpv3_tracks_backup.h5'), \n",
    "                                           'w', \n",
    "                                           obj_type='obj_type_1'\n",
    "                                           ) as writer:\n",
    "    #             writer.write_objects(objects)\n",
    "                writer.write_tracks(tracks)\n",
    "                # writer.write_segmentation(masks)\n",
    "        # Log successful completion\n",
    "\n",
    "        with btrack.io.HDF5FileHandler(f'{row, column}_cpv3_full_backup.h5', \n",
    "                                           'w', \n",
    "                                           obj_type='obj_type_1'\n",
    "                                           ) as writer:\n",
    "                    writer.write_tracks(tracks)\n",
    "                    writer.write_objects(objects)\n",
    "                    writer.write_segmentation(masks)\n",
    "        # Log successful completion\n",
    "        log_progress(acq_ID, \"Processing completed successfully\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log errors\n",
    "        log_progress(acq_ID, f\"Processing failed: {str(e)}\")\n",
    "\n",
    "# You can also log information before and after the loop\n",
    "logging.info(\"Processing completed\")\n",
    "\n",
    "# Notify if required\n",
    "notify.send_sms(\"Processing completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107ca604-b931-4c2b-a542-40af5bb4f4a4",
   "metadata": {},
   "source": [
    "# And now try and redo ps00000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171ab692-2e94-46f3-b6f1-8a2a9581ff19",
   "metadata": {},
   "source": [
    "### Load experiment of choice\n",
    "\n",
    "The Opera Phenix is a high-throughput confocal microscope that acquires very large 5-dimensional (TCZXY) images over several fields of view in any one experiment. Therefore, a lazy-loading approach is chosen to mosaic, view and annotate these images. This approach depends upon Dask and DaskFusion. The first step is to load the main metadata file (typically called `Index.idx.xml` and located in the main `Images` directory) that contains the image filenames and associated TCXZY information used to organise the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bb2069-1d41-46cc-bc8e-29f8d6ff6f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/mnt/DATA/macrohet/PS0000/'\n",
    "metadata_fn = os.path.join(base_dir, 'acquisition/Images/Index.idx.xml')\n",
    "metadata = dataio.read_harmony_metadata(metadata_fn)  \n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56301e5b-bbfa-45e7-9c14-d04f3b62dc89",
   "metadata": {},
   "source": [
    "### View assay layout and mask information (optional)\n",
    "\n",
    "The Opera Phenix acquires many time lapse series from a range of positions. The first step is to inspect the image metadata, presented in the form of an `Assaylayout/experiment_ID.xml` file, to show which positions correspond to which experimental assays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06888b8-1390-4372-b981-59226b34f1f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata_path = glob.glob(os.path.join(base_dir, 'acquisition/Assaylayout/*.xml'))[0]\n",
    "assay_layout = dataio.read_harmony_metadata(metadata_path, assay_layout=True,)# mask_exist=True,  image_dir = image_dir, image_metadata = metadata)\n",
    "assay_layout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a17cfb-8ee0-4464-8127-7d8f45925ab3",
   "metadata": {},
   "source": [
    "# Segment, localise and track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c5d2b-0b0b-4354-8ccc-71c0682c7f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtb_channel = 1\n",
    "gfp_channel = 2\n",
    "manual_mtb_thresh_channel = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6e12ed-8c64-405e-b481-a59d745f5963",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inside your loop, use the log_progress function to log progress and errors\n",
    "for (row, column), info in tqdm(assay_layout.iterrows(), desc='Progress through positions', total=len(assay_layout)):\n",
    "    try:\n",
    "        acq_ID = (row, column)\n",
    "        log_progress(acq_ID, \"Starting new acquisition\")\n",
    "        \n",
    "        # if info['Strain'] == 'UNI':\n",
    "        #     log_progress(acq_ID, \"Skipping uninfected acquisition for now\")\n",
    "        #     continue\n",
    "        # if acq_ID in already_processed_acq_IDs:\n",
    "        #     log_progress(acq_ID, \"Skipping already processed\")\n",
    "        #     continue\n",
    "        if os.path.exists(os.path.join(base_dir, f'labels/macrohet_seg_model/{row, column}_first_pass_warea.h5')):\n",
    "            log_progress(acq_ID, \"Skipping already processed\")\n",
    "            continue\n",
    "\n",
    "        # process images using zarr\n",
    "        # image_dir = os.path.join(base_dir, f'acquisition/zarr/{acq_ID}.zarr')\n",
    "        # zarr_store = zarr.open(image_dir, mode='r')\n",
    "        # images = zarr_store.images\n",
    "        # # create a max projection\n",
    "        # images = np.max(images, axis = 2)\n",
    "\n",
    "        image_dir = os.path.join(base_dir, 'acquisition/Images')\n",
    "        images = tile.compile_mosaic(image_dir, \n",
    "                                     metadata, \n",
    "                                     row, column, \n",
    "                                     # subset_field_IDs=['16', '17',  '20', '21'], \n",
    "                                     # n_tile_rows = 2, n_tile_cols = 2,\n",
    "                                     set_plane='max_proj'\n",
    "                                     # set_channel=1,\n",
    "                                     # set_time = 1,\n",
    "        #                             input_transforms = [input_transforms]\n",
    "                                    ).compute().compute()\n",
    "        images = images[:,:,0,...]\n",
    "        \n",
    "        log_progress(acq_ID, \"Images loaded and stacked\")\n",
    "\n",
    "        # check if already segmented using m2 model\n",
    "    #     if os.path.exists(os.path.join(base_dir, f'labels/macrohet_seg_model/{row, column}.h5')):\n",
    "    #         continue\n",
    "    #     else:\n",
    "        log_progress(acq_ID, \"Starting segmentation\")\n",
    "\n",
    "        if os.path.exists(os.path.join(base_dir, f'labels/macrohet_seg_model/{row, column}_first_pass_seg_backup.h5')):\n",
    "            with btrack.io.HDF5FileHandler(os.path.join(base_dir, f'labels/macrohet_seg_model/{row, column}_first_pass_seg_backup.h5'), \n",
    "                                           'r', \n",
    "                                           obj_type='obj_type_1'\n",
    "                                           ) as reader:\n",
    "    #             writer.write_objects(objects)\n",
    "                # writer.write_tracks(tracks)\n",
    "                masks = reader.segmentation\n",
    "            log_progress(acq_ID, \"Loaded previously calculated segmentation\")\n",
    "        else:\n",
    "            # segment images from gfp channel only\n",
    "            masks = np.stack([segment(frame) \n",
    "                              for frame in tqdm(images[:,gfp_channel,...],  # segmenting the GFP channel \n",
    "                                                desc = 'Segmenting')])\n",
    "\n",
    "            log_progress(acq_ID, \"Finished segmentation\")\n",
    "        with btrack.io.HDF5FileHandler(os.path.join(base_dir, f'labels/macrohet_seg_model/{row, column}_first_pass_seg_backup.h5'), \n",
    "                                           'w', \n",
    "                                           obj_type='obj_type_1'\n",
    "                                           ) as writer:\n",
    "    #             writer.write_objects(objects)\n",
    "                # writer.write_tracks(tracks)\n",
    "                writer.write_segmentation(masks)\n",
    "        log_progress(acq_ID, \"Saved out masks\")  \n",
    "        log_progress(acq_ID, \"Measuring Mtb area\")       \n",
    "        # characterise Mtb growth using Otsu segmentation\n",
    "        # otsu_mtb = otsu_threshold(images[:,1,...]) # time consuming and non-deterministic when compared to hardcoded, could result in different thresholds for same image? \n",
    "        # characterise Mtb growth using hardcoded threshold :S\n",
    "        manual_mtb_thresh = np.where(images[:,mtb_channel,...] >= Mtb_load_thresh, 1, 0)\n",
    "        log_progress(acq_ID, \"Creating intensity image for localisation\")  \n",
    "        # reshape intensity image to be gfp, rfp on last axis for regionprops\n",
    "        intensity_image = np.stack([images[:,0,...], \n",
    "                                    images[:,1,...],  \n",
    "    #                                 otsu_mtb, \n",
    "                                    manual_mtb_thresh], axis = -1)\n",
    "        log_progress(acq_ID, \"Localising objects\")  \n",
    "        # localise objects\n",
    "        objects = localise(masks, \n",
    "                           intensity_image, \n",
    "                           )\n",
    "        log_progress(acq_ID, \"Filtering small objects\")  \n",
    "        # filter out small objects\n",
    "        objects = [o for o in objects if o.properties['area'] > segment_size_thresh]\n",
    "\n",
    "        log_progress(acq_ID, \"Adding infection labels to objects\")  \n",
    "        # add label for infection\n",
    "        for obj in objects:\n",
    "            obj.properties = ({\"Infected\": True} \n",
    "                                if obj.properties['mean_intensity'][manual_mtb_thresh_channel] > 0 # index 2 for manual mtb channel \n",
    "                                else {\"Infected\": False})\n",
    "\n",
    "        log_progress(acq_ID, \"Beginning tracking\")  \n",
    "        # track on upscaled config fn\n",
    "        tracks = track(objects, masks, config_fn, search_radius = 20)\n",
    "        log_progress(acq_ID, \"Saving tracking\")  \n",
    "        # save out \n",
    "        with btrack.io.HDF5FileHandler(os.path.join(base_dir, f'labels/macrohet_seg_model/{row, column}_first_pass_warea.h5'), \n",
    "                                           'w', \n",
    "                                           obj_type='obj_type_1'\n",
    "                                           ) as writer:\n",
    "    #             writer.write_objects(objects)\n",
    "                writer.write_tracks(tracks)\n",
    "                writer.write_segmentation(masks)\n",
    "        # Log successful completion\n",
    "        log_progress(acq_ID, \"Processing completed successfully\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log errors\n",
    "        log_progress(acq_ID, f\"Processing failed: {str(e)}\")\n",
    "\n",
    "# You can also log information before and after the loop\n",
    "logging.info(\"Processing completed\")\n",
    "\n",
    "# Notify if required\n",
    "# notify.send_sms(\"Processing completed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egg",
   "language": "python",
   "name": "egg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
