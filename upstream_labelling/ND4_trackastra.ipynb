{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a60b3d0-6a76-4657-afd9-bccbeab47fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Wed_Nov_22_10:17:15_PST_2023\n",
      "Cuda compilation tools, release 12.3, V12.3.107\n",
      "Build cuda_12.3.r12.3/compiler.33567101_0\n",
      "Fri Oct 25 14:41:07 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.42.06              Driver Version: 555.42.06      CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               Off |   00000000:65:00.0  On |                  Off |\n",
      "| 30%   42C    P8             35W /  300W |   48403MiB /  49140MiB |     20%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2523      G   /usr/lib/xorg/Xorg                            486MiB |\n",
      "|    0   N/A  N/A      3966      G   /usr/bin/gnome-shell                          241MiB |\n",
      "|    0   N/A  N/A    427336      G   ...AAAAAAAACAAAAAAAAAA= --shared-files        139MiB |\n",
      "|    0   N/A  N/A   1635241      G   ...seed-version=20240827-180123.409000        145MiB |\n",
      "|    0   N/A  N/A   1635404      G   ...yOnDemand --variations-seed-version         25MiB |\n",
      "|    0   N/A  N/A   1641094      G   ...miniconda3/envs/godspeed/bin/python          4MiB |\n",
      "|    0   N/A  N/A   1645391    C+G   ...miniconda3/envs/godspeed/bin/python      47275MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cellpose.core:Neither TORCH CUDA nor MPS version not installed/working.\n",
      "INFO:cellpose.core:Neither TORCH CUDA nor MPS version not installed/working.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> GPU activated? NO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cellpose.core:>>>> using CPU\n",
      "INFO:cellpose.core:>>>> using CPU\n",
      "INFO:cellpose.models:>> cyto << model set to be used\n",
      "INFO:cellpose.core:WARNING: MKL version on torch not working/installed - CPU version will be slightly slower.\n",
      "INFO:cellpose.core:see https://pytorch.org/docs/stable/backends.html?highlight=mkl\n",
      "INFO:cellpose.models:>>>> loading model /home/dayn/.cellpose/models/cytotorch_0\n",
      "INFO:cellpose.models:>>>> model diam_mean =  30.000 (ROIs rescaled to this size during training)\n",
      "INFO:trackastra.model.model:Loading model state from /home/dayn/.trackastra/.models/general_2d/model.pt\n",
      "INFO:trackastra.model.model_api:Using device cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dayn/.trackastra/.models/general_2d already downloaded, skipping.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Load a pretrained model\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m trackastra_model \u001b[38;5;241m=\u001b[39m \u001b[43mTrackastra\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgeneral_2d\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/godspeed/lib/python3.10/site-packages/trackastra/model/model_api.py:85\u001b[0m, in \u001b[0;36mTrackastra.from_pretrained\u001b[0;34m(cls, name, device, download_dir)\u001b[0m\n\u001b[1;32m     83\u001b[0m folder \u001b[38;5;241m=\u001b[39m download_pretrained(name, download_dir)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# download zip from github to location/name, then unzip\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/godspeed/lib/python3.10/site-packages/trackastra/model/model_api.py:76\u001b[0m, in \u001b[0;36mTrackastra.from_folder\u001b[0;34m(cls, dir, device)\u001b[0m\n\u001b[1;32m     74\u001b[0m transformer \u001b[38;5;241m=\u001b[39m TrackingTransformer\u001b[38;5;241m.\u001b[39mfrom_folder(\u001b[38;5;28mdir\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m train_args \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mdir\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_config.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m), Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/godspeed/lib/python3.10/site-packages/trackastra/model/model_api.py:68\u001b[0m, in \u001b[0;36mTrackastra.__init__\u001b[0;34m(self, transformer, train_args, device)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevice \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not recognized.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_args \u001b[38;5;241m=\u001b[39m train_args\n",
      "File \u001b[0;32m~/miniconda3/envs/godspeed/lib/python3.10/site-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/godspeed/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/godspeed/lib/python3.10/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/godspeed/lib/python3.10/site-packages/torch/nn/modules/module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1154\u001b[0m             device,\n\u001b[1;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m             non_blocking,\n\u001b[1;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1158\u001b[0m         )\n\u001b[0;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import btrack\n",
    "import zarr\n",
    "import os\n",
    "import napari\n",
    "from macrohet import dataio\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from trackastra.model import Trackastra\n",
    "from trackastra.tracking import graph_to_ctc, graph_to_napari_tracks\n",
    "import h5py\n",
    "\n",
    "!nvcc --version\n",
    "!nvidia-smi\n",
    "\n",
    "from cellpose import core, utils, io, models, metrics\n",
    "use_GPU = core.use_gpu()\n",
    "yn = ['NO', 'YES']\n",
    "print(f'>>> GPU activated? {yn[use_GPU]}')\n",
    "cellpose_model = models.Cellpose(gpu=True, model_type='cyto')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Load a pretrained model\n",
    "trackastra_model = Trackastra.from_pretrained(\"general_2d\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d6fbe32-5e33-49a0-98c9-642aaf3b5839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading metadata XML file...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77774ac69174428a9ef95c8516cbe4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting metadata complete!\n",
      "Reading metadata XML file...\n",
      "Extracting metadata complete!\n"
     ]
    }
   ],
   "source": [
    "expt_ID = 'ND0004'\n",
    "base_dir = f'/mnt/SYNO/macrohet_syno/data/{expt_ID}/'\n",
    "metadata_fn = glob.glob(os.path.join(base_dir, 'acquisition/Images/Index*xml'))[0]\n",
    "metadata = dataio.read_harmony_metadata(metadata_fn)  \n",
    "metadata_path = glob.glob(os.path.join(base_dir, 'acquisition/Assaylayout/*.xml'))[0]\n",
    "assay_layout = dataio.read_harmony_metadata(metadata_path, assay_layout=True,replicate_number=True)# mask_exist=True,  image_dir = image_dir, image_metadata = metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef9a4779-ec73-4bb5-aca5-4e5cc72790af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'segmentation' at 0x7f3e049b6e90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer = napari.Viewer(title = 'testing nd4 segmentation and tracking')\n",
    "\n",
    "viewer.add_image(images, channel_axis = 1, \n",
    "                 colormap=['magenta', 'green'],\n",
    "                 blending = 'additive', \n",
    "                 contrast_limits=[[0, 1000], [0, 2400]])\n",
    "viewer.add_labels(segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cc9b8ef-cb1b-4ac8-8d53-d74c049156ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = (1,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5764b6-d9d5-423c-b05a-e77ced5b4dfe",
   "metadata": {},
   "source": [
    "# uniting z3 cellpose and trackastra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3156d406-c54b-4b5a-a64e-4ef2f6ea6bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define thresholds\n",
    "segment_size_thresh = 5000\n",
    "mtb_load_thresh = 480\n",
    "\n",
    "mtb_channel_ID = 0\n",
    "mphi_channel_ID = 1\n",
    "\n",
    "def calculate_msd(x, y):\n",
    "    # Calculate the displacement between successive frames\n",
    "    dx = np.diff(x, prepend=x[0])\n",
    "    dy = np.diff(y, prepend=y[0])\n",
    "    msd = np.sqrt(dx**2 + dy**2)\n",
    "    return msd\n",
    "\n",
    "# Assuming you have an 'image' that corresponds to your segmentation\n",
    "def measure_segment_intensity(image, segmentation, ID):\n",
    "    \"\"\"Measure intensity of pixels under a specific segment in the image.\"\"\"\n",
    "    mask = segmentation == ID  # Create a mask for the segment with 'seg_id'\n",
    "    segment_intensity = image[mask]  # Extract pixel values under the segment mask\n",
    "    return np.mean(segment_intensity) #, np.sum(segment_intensity)  # Example measurements\n",
    "\n",
    "\n",
    "expt_ID = 'ND0004'\n",
    "base_dir = f'/mnt/SYNO/macrohet_syno/data/{expt_ID}/'\n",
    "metadata_fn = glob.glob(os.path.join(base_dir, 'acquisition/Images/Index*xml'))[0]\n",
    "metadata = dataio.read_harmony_metadata(metadata_fn)  \n",
    "metadata_path = glob.glob(os.path.join(base_dir, 'acquisition/Assaylayout/*.xml'))[0]\n",
    "assay_layout = dataio.read_harmony_metadata(metadata_path, assay_layout=True,replicate_number=True)# mask_exist=True,  image_dir = image_dir, image_metadata = metadata)\n",
    "image_resolution = float(metadata['ImageResolutionX'].iloc[0])\n",
    "meters_area_per_pixel = image_resolution**2\n",
    "mum_sq_scale_factor = (1E-6)**2\n",
    "pixel_to_mum_sq_scale_factor = meters_area_per_pixel/mum_sq_scale_factor\n",
    "\n",
    "\n",
    "# Sort by custom order of the Compound and ConcentrationEC\n",
    "compound_order = ['RIF', 'PZA', 'INH', 'CTRL', 'BDQ']\n",
    "concentration_order = ['EC99', 'EC50', 'EC0']\n",
    "\n",
    "# Define custom sort logic for the DataFrame\n",
    "assay_layout['compound_sort'] = assay_layout['Compound'].apply(lambda x: compound_order.index(x) if x in compound_order else len(compound_order))\n",
    "assay_layout['concentration_sort'] = assay_layout['ConcentrationEC'].apply(lambda x: concentration_order.index(x) if x in concentration_order else len(concentration_order))\n",
    "assay_layout['strain_sort'] = assay_layout['Strain'].apply(lambda x: 0 if x == 'WT' else (1 if x == 'RD1' else 2))\n",
    "\n",
    "# Sort the DataFrame based on the defined sort order\n",
    "assay_layout_sorted = assay_layout.sort_values(by=['concentration_sort', 'compound_sort', 'strain_sort'])\n",
    "\n",
    "# Extract the row-column tuples from the sorted DataFrame index\n",
    "row_col_order = list(assay_layout_sorted.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e328525-bc56-4016-830b-cd7bbd1d7834",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for acq_ID in tqdm(row_col_order, total = len(row_col_order), desc = 'Iterating over individual wells'):\n",
    "    \n",
    "    if os.path.exists(f'/mnt/SYNO/macrohet_syno/data/{expt_ID}/labels/sc_df_{acq_ID[0]}.{acq_ID[1]}.{expt_ID}.pkl'):\n",
    "        print(f'skipping acq ID {acq_ID}')\n",
    "        continue\n",
    "\n",
    "    if acq_ID == (4, 7):\n",
    "        continue\n",
    "        \n",
    "    #technical replicate\n",
    "    technical_replicate = assay_layout.loc[(acq_ID[0], acq_ID[1])]['Replicate #']\n",
    "    #biological replicate\n",
    "    biological_replicate = 4\n",
    "    #strain\n",
    "    strain = assay_layout.loc[(acq_ID[0], acq_ID[1])]['Strain']\n",
    "    #compound\n",
    "    compound = assay_layout.loc[(acq_ID[0], acq_ID[1])]['Strain']\n",
    "    #concentration\n",
    "    concentration = assay_layout.loc[(acq_ID[0], acq_ID[1])]['Concentration']\n",
    "    # load images and max project them\n",
    "    image_dir = os.path.join(base_dir, f'acquisition/zarr/{acq_ID}.zarr')\n",
    "    zarr_group = zarr.open(image_dir, mode='r')\n",
    "    images = zarr_group.images[...]\n",
    "    # load the original cellpose segmentation\n",
    "    with btrack.io.HDF5FileHandler(os.path.join(f'/mnt/SYNO/macrohet_syno/data/{expt_ID}/labels/cpv3/{acq_ID}.h5'), #macrohet_seg_model \n",
    "                                               'r', \n",
    "                                               obj_type='obj_type_1'\n",
    "                                               ) as reader:\n",
    "                    original_segmentation = reader.segmentation\n",
    "    # calculate the new cellpose segmentation\n",
    "    segmentation_input = images[:,1,2,...]\n",
    "    mask_stack = []\n",
    "    for frame in tqdm(segmentation_input, total = len(segmentation_input)):\n",
    "        masks, flows, styles, diams = cellpose_model.eval(frame, \n",
    "                                                 diameter=150, \n",
    "                                                 channels=[0,0],)\n",
    "    \n",
    "        mask_stack.append(masks)\n",
    "    tracking_input_segmentation = np.stack(mask_stack, axis = 0)\n",
    "    # perform checks on segmentation\n",
    "    \n",
    "    # Check if the first frame is blank and, if so, copy from the first non-blank frame\n",
    "    if np.all(tracking_input_segmentation[0] == 0):\n",
    "        # Find the first non-blank frame\n",
    "        first_non_blank_found = False\n",
    "        for j in range(1, tracking_input_segmentation.shape[0]):\n",
    "            if not np.all(tracking_input_segmentation[j] == 0):  # Check if frame j is non-blank\n",
    "                tracking_input_segmentation[0] = tracking_input_segmentation[j]\n",
    "                first_non_blank_found = True\n",
    "                print(f\"First frame was blank. Copied from frame {j}.\")\n",
    "                break\n",
    "        if not first_non_blank_found:\n",
    "            error_log.append(\"All frames are blank. Skipping segmentation processing.\")\n",
    "            print(\"Error: All frames are blank. No processing will be done.\")\n",
    "            # Exit if all frames are blank since there's nothing to process\n",
    "            raise ValueError(\"Segmentation data is entirely blank.\")\n",
    "     # Iterate over frames and check for blank frames, starting from the second frame\n",
    "    for i in range(1, tracking_input_segmentation.shape[0]):\n",
    "        if np.all(tracking_input_segmentation[i] == 0):  # Check if the current frame is blank\n",
    "            # Copy the previous non-blank frame if available\n",
    "            if np.all(tracking_input_segmentation[i-1] == 0):\n",
    "                error_log.append(f\"Frame {i} and previous frames are blank, unable to copy.\")\n",
    "                print(f\"Error: Frame {i} is blank and cannot be copied from previous frames.\")\n",
    "                continue\n",
    "            else:\n",
    "                tracking_input_segmentation[i] = tracking_input_segmentation[i-1]\n",
    "                print(f\"Frame {i} was blank. Copied from frame {i-1}.\")\n",
    "        else:\n",
    "            # Track the last non-blank frame as we go\n",
    "            last_non_blank_frame = tracking_input_segmentation[i]\n",
    "    # Log the error messages and proceed with the loop\n",
    "    if error_log:\n",
    "        for error in error_log:\n",
    "            print(error)\n",
    "        print(\"Segmentation completed with some frames skipped due to consecutive blank frames.\")\n",
    "\n",
    "    # Assuming 'segmentation' is your numpy array\n",
    "    with h5py.File(f'/mnt/SYNO/macrohet_syno/data/{expt_ID}/labels/z3_cpv3/{acq_ID}.h5', 'w') as f:\n",
    "        # Save the array with compression for efficient storage\n",
    "        f.create_dataset('segmentation', data=tracking_input_segmentation, compression=\"gzip\")\n",
    "    # track using new segmentation\n",
    "    # Track the cells\n",
    "    track_graph = trackastra_model.track(segmentation_input, tracking_input_segmentation, mode=\"greedy_nodiv\")  # or mode=\"ilp\", or \"greedy_nodiv\"\n",
    "    \n",
    "    # Visualise in napari\n",
    "    tracks, napari_tracks_graph, _ = graph_to_napari_tracks(track_graph)\n",
    "    tracks = pd.DataFrame(tracks, columns=['ID', 't', 'x', 'y']).astype(int)\n",
    "    # Filter tracks with length greater than 75\n",
    "    tracks = tracks.groupby('ID').filter(lambda x: len(x) > 75)\n",
    "    \n",
    "    # split channels\n",
    "    mphi_channel = np.max(images[:,mphi_channel_ID,...], axis=1)\n",
    "    mtb_channel = np.max(images[:,mtb_channel_ID,...], axis = 1)\n",
    "    thresholded_mtb_channel = mtb_channel >= mtb_load_thresh\n",
    "    track_dfs = []\n",
    "    # now measure properties from prior segmentation?\n",
    "    for cell_ID, track in tqdm(tracks.groupby('ID'), desc = 'iterating over tracks', total = len(tracks.ID.unique()), leave = False):\n",
    "        track = track.sort_values(by='t')\n",
    "        # Extract coordinates and time\n",
    "        times = track['t'].to_numpy() / 2  # Assuming you want to halve the time\n",
    "        frames = track['t'].to_numpy() \n",
    "    \n",
    "        x_coords = track['x'].to_numpy().astype(int)\n",
    "        y_coords = track['y'].to_numpy().astype(int)\n",
    "    \n",
    "    \n",
    "        mtb_areas = []\n",
    "        mphi_areas = []\n",
    "        mean_intensities = []\n",
    "        # calculate the mtb pixel area and µm area\n",
    "        for i, frame in tqdm(enumerate(frames), desc = f'Iterating over frames for track ID {cell_ID}', total = len(frames), leave = False):\n",
    "            frame = frame - 1\n",
    "            segmentation_input_ID = original_segmentation[frame][x_coords[i], y_coords[i]]\n",
    "            if segmentation_input_ID == 0:  # Ignore background (assuming 0 is the background ID)\n",
    "                    mtb_area_pixels = np.nan\n",
    "                    mphi_area_pixels = np.nan\n",
    "            else:\n",
    "                \n",
    "                mask = original_segmentation[frame] == segmentation_input_ID  # Create a mask for the segment with 'seg_id'\n",
    "                \n",
    "                # chop up images into segments here\n",
    "                # image_segment = images[frame][:][mask]\n",
    "                thresholded_image_segment = thresholded_mtb_channel[frame][mask]\n",
    "        \n",
    "                # meaure segment\n",
    "                mtb_area_pixels = np.sum(thresholded_image_segment)\n",
    "                mphi_area_pixels = np.sum(mask)\n",
    "                # mean_intensity = np.mean(image_segment)\n",
    "    \n",
    "            # store measurements\n",
    "            mtb_areas.append(mtb_area_pixels)\n",
    "            mphi_areas.append(mphi_area_pixels)\n",
    "            # mean_intensities.append(mean_intensity)\n",
    "    \n",
    "        track['Mtb Area (µm)'] = np.array(mtb_areas) * pixel_to_mum_sq_scale_factor\n",
    "        track['Mphi Area (µm)'] = np.array(mphi_areas) * pixel_to_mum_sq_scale_factor\n",
    "        # track['RFP'] = mean_intensities[mtb_channel_ID]\n",
    "        # track['GFP'] = mean_intensities[mphi_channel_ID]\n",
    "        # Compute MSD in a vectorized way\n",
    "        # track['MSD'] = calculate_msd(x_coords, y_coords)\n",
    "        \n",
    "        # infection statuses\n",
    "        track['Infection Status'] = track['Mtb Area (µm)'] > 0\n",
    "    \n",
    "        track['Initial Infection Status'] = track['Mtb Area (µm)'].iloc[0] > 0 \n",
    "        track['Final Infection Status'] = track['Mtb Area (µm)'].iloc[-1] > 0 \n",
    "        track['ID'] = cell_ID\n",
    "        track['Unique_ID'] = f'{cell_ID}.{acq_ID[0]}.{acq_ID[1]}.{expt_ID}'\n",
    "    \n",
    "        track_dfs.append(track)\n",
    "    \n",
    "    # Concatenate all track DataFrames into the larger 'df' DataFrame\n",
    "    df = pd.concat(track_dfs, ignore_index=True)\n",
    "    \n",
    "    df.to_pickle(f'/mnt/SYNO/macrohet_syno/data/{expt_ID}/labels/sc_df_{acq_ID[0]}.{acq_ID[1]}.{expt_ID}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942f6bef-d733-4f5f-aa77-d27e0b93830f",
   "metadata": {},
   "source": [
    "# Checking segentation matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae1461c9-20c4-408a-ab71-2be847bf7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_by = 'ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bfb57ed8-4cb4-4bff-9661-c795f7b5fb5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96100f99007240c2bb507a2a7c48f8bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterating over tracks:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### checking segentation matching\n",
    "\n",
    "# Initialize relabeled array\n",
    "relabeled = np.zeros_like(segmentation)\n",
    "\n",
    "# Iterate over each track, grouped by 'ID'\n",
    "for cell_ID, track in tqdm(tracks.groupby('ID'), desc = 'Iterating over tracks', total = len(tracks.ID.unique())):\n",
    "    track = track.sort_values(by='T')\n",
    "    times = (track['T'].to_numpy()).astype(int)  # Time (T) halved and converted to int\n",
    "    x_coords = (track['X'].to_numpy() * scale[0]).astype(int)\n",
    "    y_coords = (track['Y'].to_numpy() * scale[1]).astype(int)\n",
    "\n",
    "    # Iterate over each time point\n",
    "    for i, t in enumerate(times):\n",
    "        # Ensure we are within the segmentation time bounds\n",
    "        if t >= segmentation.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Handle 2D segmentation\n",
    "        old_id = segmentation[t][x_coords[i], y_coords[i]]\n",
    "        if old_id == 0:  # Ignore background (assuming 0 is the background ID)\n",
    "                continue\n",
    "        # Recolor segmentation by the chosen property (ID in this case)\n",
    "        old_id_mask = segmentation[t] == old_id\n",
    "        # print(old_id)\n",
    "        # Recolor all pixels in this mask with the new ID\n",
    "        new_id = int(cell_ID)\n",
    "        relabeled[t][old_id_mask] = new_id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "godspeed",
   "language": "python",
   "name": "godspeed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
