{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import dask.array as da\n",
    "from dask.array.core import normalize_chunks\n",
    "import numpy as np\n",
    "from shapely.geometry import GeometryCollection, Point\n",
    "from skimage.transform import AffineTransform\n",
    "\n",
    "from fuse.fuse import fuse_func\n",
    "from utils.download_sample import download_from_dropbox\n",
    "from utils.metadata import extract_coordinates, normalize_coords_to_pixel\n",
    "from utils.imutils import crop_black_border, load_image, transpose, remove_background\n",
    "from utils.shapely_and_napari_utils import get_transformed_array_corners, numpy_shape_to_shapely\n",
    "from utils.chunks import get_chunk_coordinates, get_rect_from_chunk_boundary, find_chunk_tile_intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import re \n",
    "import os\n",
    "from natsort import natsorted\n",
    "import enum\n",
    "import napari\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading own functions to sort files and extract metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@enum.unique\n",
    "class Channels(enum.Enum):\n",
    "    BRIGHTFIELD = 0\n",
    "    GFP = 1\n",
    "    RFP = 2\n",
    "    IRFP = 3\n",
    "    PHASE = 4\n",
    "    WEIGHTS = 50\n",
    "    MASK_IRFP = 96\n",
    "    MASK_RFP = 97\n",
    "    MASK_GFP = 98\n",
    "    MASK = 99\n",
    "    \n",
    "def parse_filename(filename: os.PathLike) -> dict:\n",
    "    \"\"\"Parse an OctopusLite filename and retreive metadata from the file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : PathLike\n",
    "        The full path to a file to parse.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    metadata : dict\n",
    "        A dictionary containing the parsed metadata.\n",
    "    \"\"\"\n",
    "    \n",
    "    OCTOPUSLITE_FILEPATTERN =(\n",
    "        \"r(?P<row>[0-9]+)c(?P<column>[0-9]+)f(?P<fov>[0-9]+)p(?P<plane>[0-9]+)-ch(?P<channel>[0-9]+)\"\n",
    "        \"sk(?P<time>[0-9]+)fk(?P<fk>[0-9]+)fl(?P<fl>[0-9]+)\"\n",
    "        )\n",
    "    \n",
    "    pth, filename = os.path.split(filename)\n",
    "    params = re.match(OCTOPUSLITE_FILEPATTERN, filename)\n",
    "\n",
    "    metadata = {\n",
    "        \"filename\": filename,\n",
    "        \"channel\": Channels(int(params.group(\"channel\"))),\n",
    "        \"time\": params.group(\"time\"),\n",
    "        \"row\": params.group(\"row\"), \n",
    "        \"column\": params.group(\"column\"), \n",
    "        \"fov\": params.group(\"fov\"), \n",
    "        \"plane\": params.group(\"plane\"), \n",
    "        \"time\": params.group(\"time\"), \n",
    "        \"fk\": params.group(\"fk\"), \n",
    "        \"fl\": params.group(\"fl\")\n",
    "\n",
    "    }\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define parameters of mosaic compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 4],\n",
       "       [7, 6, 5],\n",
       "       [8, 1, 9]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### array that shows the location of each fov in the scan pattern\n",
    "fov_scan_pattern = np.array(([2,3,4],\n",
    "                             [7,6,5],\n",
    "                             [8,1,9],))\n",
    "fov_scan_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = 0.1\n",
    "chunk_size = (108,108)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_dir = Path(\"/mnt/DATA/sandbox/pierre_live_cell_data/outputs/Replication_IPSDM_GFP/Images/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_channels = set([parse_filename(fn)['channel'] for fn in os.listdir(image_dir)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4256f56cbb14caeb3428297166b5185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel BRIGHTFIELD not found in image directory\n",
      "Starting channel GFP\n"
     ]
    }
   ],
   "source": [
    "#iterate over channels\n",
    "for ch in tqdm(Channels):#Channels:\n",
    "    if ch in relevant_channels:\n",
    "        print('Starting channel', ch.name)\n",
    "        ### define empty z stack\n",
    "        da_zt_stack = []\n",
    "        ### iterate over planes \n",
    "        for p in range(1,4):\n",
    "            ### define empty time series stack\n",
    "            da_t_stack = []\n",
    "            ### iterate over frames\n",
    "            for t in (range(1,76)):\n",
    "                ### get all files at that time point\n",
    "                files = list(Path(\"/mnt/DATA/sandbox/pierre_live_cell_data/outputs/Replication_IPSDM_GFP/Images/\").glob(f\"r03c03f*p0{p}-ch{ch.value}sk{t}fk*\"))\n",
    "                ### sort using the frame\n",
    "                files.sort(key=lambda f: int(parse_filename(f)[\"time\"]))\n",
    "                ### extract tile coordinates from fn 'f' (ie. fov number)\n",
    "                coords = pd.DataFrame()\n",
    "                for fn in files:\n",
    "                    entry = pd.DataFrame([parse_filename(fn)])\n",
    "                    coords = pd.concat([coords, entry], ignore_index=True)\n",
    "                ## lazy hack to make the raster scan like the provided fov_scan_pattern\n",
    "                for i, row in coords.iterrows():\n",
    "                    (X, Y) = tuple(map(int, np.where(fov_scan_pattern == int(row['fov']))))\n",
    "                    coords.at[i, 'X'], coords.at[i, 'Y'] = X, Y\n",
    "                ### lazy hack to register the row/col number as x/y location shifted by the amount of pixels in each image\n",
    "                coords['um/px'] = 1/(2160*(1-overlap))\n",
    "                ### turn coords into a np array of transformation amount using the um/px ratio\n",
    "                normalized_coords = normalize_coords_to_pixel(coords).to_numpy()\n",
    "                ### apply transforms if required -- could this be a background removal???\n",
    "                input_transforms = None#[remove_background] #None #[crop_black_border, ]#transpose]\n",
    "                ### define a function to load a test image and get tile shape from it\n",
    "                _load_image = partial(load_image, transforms=input_transforms)\n",
    "                tile_shape=_load_image(str(files[0])).shape\n",
    "                ### apply the transformation to each tile to correctly mosaic them\n",
    "                transforms = [AffineTransform(translation=stage_coord).params for stage_coord in normalized_coords]\n",
    "                tiles = [get_transformed_array_corners(tile_shape, transform) for transform in transforms]\n",
    "                ### define the bounding boxes of the tiles and overall FOV to determine the dask output shape\n",
    "                all_bboxes = np.vstack(tiles)\n",
    "                all_min = all_bboxes.min(axis=0)\n",
    "                all_max = all_bboxes.max(axis=0)\n",
    "                stitched_shape=tuple(np.ceil(all_max-all_min).astype(int))\n",
    "                ### if there is a discrepancy between the top left tile and the origin then shift\n",
    "                shift_to_origin = AffineTransform(translation=-all_min)\n",
    "                transforms_with_shift = [t @ shift_to_origin.params for t in transforms]\n",
    "                shifted_tiles = [get_transformed_array_corners(tile_shape, t) for t in transforms_with_shift]\n",
    "                tiles_shifted_shapely = [numpy_shape_to_shapely(s) for s in shifted_tiles]\n",
    "                ### split data into pre-defined chunks             \n",
    "                chunks = normalize_chunks(chunk_size,shape=tuple(stitched_shape))\n",
    "                computed_shape = np.array(list(map(sum, chunks)))\n",
    "                ### check that tiles shape is correct\n",
    "                assert np.all(np.array(stitched_shape) == computed_shape)\n",
    "                ## get chunk details and plot as shapes\n",
    "                chunk_boundaries = list(get_chunk_coordinates(stitched_shape, chunk_size))\n",
    "                chunk_shapes = list(map(get_rect_from_chunk_boundary, chunk_boundaries))\n",
    "                chunks_shapely = [numpy_shape_to_shapely(c) for c in chunk_shapes]\n",
    "                ### iterate over files in an individual frame and attach tile info and transform\n",
    "                for tile_shifted_shapely, file, transform in zip(tiles_shifted_shapely, \n",
    "                                                                 files, \n",
    "                                                                 transforms_with_shift):\n",
    "                    tile_shifted_shapely.fuse_info = {'file':file, 'transform':transform}\n",
    "                ### iterate over chunks for a single image and attach info\n",
    "                for chunk_shapely, chunk_boundary  in zip(chunks_shapely, chunk_boundaries):\n",
    "                    chunk_shapely.fuse_info = {'chunk_boundary': chunk_boundary}\n",
    "                ### find intersection of tiles and chunks\n",
    "                chunk_tiles = find_chunk_tile_intersections(tiles_shifted_shapely, chunks_shapely)\n",
    "                ### define a fuse function to load all tiles for particular chunk\n",
    "                _fuse_func=partial(fuse_func, \n",
    "                                   imload_fn=_load_image,\n",
    "                                   dtype=np.uint16) \n",
    "                ### use map_blocks to calculate the fused image chunk by chunk\n",
    "                target_array = da.map_blocks(func=_fuse_func,\n",
    "                                             chunks=chunks, \n",
    "                                             input_tile_info=chunk_tiles,\n",
    "                                             dtype=np.uint16)\n",
    "                ### append the mosaic for that particular frame to a list of mosaics\n",
    "                da_t_stack.append(target_array)\n",
    "            ### stack that mosaic in a time series\n",
    "            da_t_stack = da.stack(da_t_stack, axis = 0)\n",
    "            ### append the time series for one plane to the z-stack\n",
    "            da_zt_stack.append(da_t_stack)\n",
    "        ### stack the z planes together\n",
    "        da_zt_stack = da.stack(da_zt_stack, axis = 0)\n",
    "        ### save out as zarr\n",
    "        da_zt_stack.to_zarr(f\"data/zt_stack_ch{ch.value}_108tile.zarr\", overwrite=True)\n",
    "    else:\n",
    "        print(f'Channel {ch.name} not found in image directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Image layer 'rfp' at 0x7fdf4fbc3e80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da_stack_gfp = da.from_zarr(f\"zt_stack_ch{1}.zarr\")\n",
    "da_stack_rfp = da.from_zarr(f\"zt_stack_ch{2}.zarr\")\n",
    "v = napari.Viewer()\n",
    "v.add_image(da_stack_gfp, name=\"gfp\", contrast_limits = [0,2352], blending = 'additive', colormap= 'green')# colormap = 'g')\n",
    "v.add_image(da_stack_rfp, name=\"rfp\", contrast_limits = [103,164], blending = 'additive', colormap = 'magenta')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8c1da9b333bd16eca91c483de28cf1e3bdef92ae84368abdb550f55cbacd6d65"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
