{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d75a8321",
   "metadata": {},
   "source": [
    "# Implementing tile overlap\n",
    "\n",
    "The example that this process is based off is only 2D image. \n",
    "\n",
    "I have a 4D stack so I need to separate each layer and tile individually.\n",
    "\n",
    "Now tiling over time and z stack, but testing to see if order of iteration matters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f9a3f20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import enum\n",
    "import re\n",
    "import dask\n",
    "import dask.array as da\n",
    "from dask.array.core import normalize_chunks\n",
    "from dask_image.imread import imread\n",
    "import skimage\n",
    "from skimage.io import imshow #, imread\n",
    "from dask_image.imread import imread\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import napari\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from skimage.transform import AffineTransform\n",
    "from typing import Tuple, List, Dict, Union, Optional, Callable \n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from scipy.ndimage import affine_transform\n",
    "\n",
    "from shapely.geometry.base import BaseGeometry\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from shapely.geometry import LineString\n",
    "from shapely.strtree import STRtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0c90eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@enum.unique\n",
    "class Channels(enum.Enum):\n",
    "    BRIGHTFIELD = 0\n",
    "    GFP = 1\n",
    "    RFP = 2\n",
    "    IRFP = 3\n",
    "    MASK_2 = 97\n",
    "    MASK_1 = 98\n",
    "    MASK = 99\n",
    "\n",
    "def parse_filename(filename: os.PathLike) -> dict:\n",
    "    \"\"\"Parse an OctopusHeavy filename and retreive metadata from the file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : PathLike\n",
    "        The full path to a file to parse.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    metadata : dict\n",
    "        A dictionary containing the parsed metadata.\n",
    "    \"\"\"\n",
    "    \n",
    "    OCTOPUSHEAVY_FILEPATTERN =(\n",
    "        \"r(?P<row>[0-9]+)c(?P<column>[0-9]+)f(?P<mosaic_pos>[0-9]+)p(?P<plane>[0-9]+)-ch(?P<channel>[0-9]+)\"\n",
    "        \"sk(?P<time>[0-9]+)fk(?P<fk>[0-9]+)fl(?P<fl>[0-9]+)\"\n",
    "        )\n",
    "    \n",
    "    path, filename = os.path.split(filename)\n",
    "    params = re.match(OCTOPUSHEAVY_FILEPATTERN, filename)\n",
    "\n",
    "    filename_metadata = {\n",
    "        \"filename\": filename,\n",
    "        \"channel\": params.group(\"channel\"),# Channels(int(params.group(\"channel\"))),\n",
    "        \"time\": params.group(\"time\"),\n",
    "        \"row\": params.group(\"row\"), \n",
    "        \"column\": params.group(\"column\"), \n",
    "        \"mosaic_pos\": params.group(\"mosaic_pos\"), \n",
    "        \"plane\": params.group(\"plane\"), \n",
    "        \"fk\": params.group(\"fk\"), \n",
    "        \"fl\": params.group(\"fl\")\n",
    "\n",
    "    }\n",
    "\n",
    "    return filename_metadata\n",
    "\n",
    "def read_harmony_metadata(metadata_path: os.PathLike):\n",
    "    \"\"\"\n",
    "    Read the metadata from the Harmony software for the Opera Phenix microscope.\n",
    "    Takes an input of the path to the metadata .xml file.\n",
    "    Returns the metadata in a pandas dataframe format.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### read xml metadata file\n",
    "    xml_data = open(metadata_fn, 'r').read()\n",
    "    root = ET.XML(xml_data) \n",
    "    ### extract the metadata from the xml file\n",
    "    images_metadata = [child for child in root if \"Images\" in child.tag][0]\n",
    "    ### create an empty list for storing individual image metadata\n",
    "    all_images_dicts = list()\n",
    "    ### iterate over every image entry extracting the metadata\n",
    "    for image_metadata in tqdm(images_metadata, total = len(images_metadata)):\n",
    "        ### create empty dict to store single image metadata\n",
    "        single_image_dict = dict()\n",
    "        ### iterate over every metadata item in that image metadata\n",
    "        for item in image_metadata:\n",
    "            ### get column names from metadata\n",
    "            col = item.tag.replace('{http://www.perkinelmer.com/PEHH/HarmonyV5}','')\n",
    "            ### get metadata\n",
    "            entry = item.text\n",
    "            ### make dictionary out of metadata\n",
    "            single_image_dict[col] = entry\n",
    "        ### append that image metadata to list of all images    \n",
    "        all_images_dicts.append(single_image_dict)\n",
    "    ### create a dataframe out of all metadata\n",
    "    df = pd.DataFrame(all_images_dicts)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "950e0bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_tile_coord(shape: Tuple[int,int], affine_matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    returns the corner coordinates of a 2D array with shape shape\n",
    "    after applying the transform represented by affine_matrix.\n",
    "    From DaskFusion (https://github.com/VolkerH/DaskFusion/)\n",
    "    \"\"\"\n",
    "    h, w = shape\n",
    "    # create homogeneous coordinates for corner points\n",
    "    baserect = np.array([[0, 0], [h, 0], [h, w], [0, w]])\n",
    "    augmented_baserect = np.concatenate(\n",
    "        (baserect, np.ones((baserect.shape[0], 1))), axis=1\n",
    "    )\n",
    "    # see where the corner points map to\n",
    "    transformed_rect = (affine_matrix @ augmented_baserect.T).T[:, :-1]\n",
    "    return transformed_rect\n",
    "\n",
    "def get_chunk_coord(shape: Tuple[int, int], chunk_size: Tuple[int, int]):\n",
    "    \"\"\"Iterator that returns the bounding coordinates\n",
    "    for the individual chunks of a dask array of size\n",
    "    shape with chunk size chunk_size.\n",
    "\n",
    "\n",
    "    return_np_slice determines the output format. If True,\n",
    "    a numpy slice object is returned for each chunk, that can be used\n",
    "    directly to slice a dask array to return the desired chunk region.\n",
    "    If False, a Tuple of Tuples ((row_min, row_max+1),(col_min, col_max+1))\n",
    "    is returned.\n",
    "    From DaskFusion (https://github.com/VolkerH/DaskFusion/)\n",
    "\n",
    "    \"\"\"\n",
    "    chunksy, chunksx = normalize_chunks(chunk_size, shape=shape)\n",
    "    y = 0\n",
    "    for cy in chunksy:\n",
    "        x = 0\n",
    "        for cx in chunksx:\n",
    "            yield ((y, y + cy), (x, x + cx))\n",
    "            x = x + cx\n",
    "        y = y + cy\n",
    "        \n",
    "def numpy_shape_to_shapely(coords: np.ndarray, shape_type: str = \"polygon\") -> BaseGeometry:\n",
    "    \"\"\"\n",
    "    Convert an individual shape represented as a numpy array of coordinates\n",
    "    to a shapely object\n",
    "    From DaskFusion (https://github.com/VolkerH/DaskFusion/)\n",
    "    \"\"\"\n",
    "    _coords = coords[:, ::-1].copy()  # shapely has col,row order, numpy row,col\n",
    "    _coords[:, 1] *= -1  # axis direction flipped between shapely and napari\n",
    "    if shape_type in (\"rectangle\", \"polygon\", \"ellipse\"):\n",
    "        return Polygon(_coords)\n",
    "    elif shape_type in (\"line\", \"path\"):\n",
    "        return LineString(_coords)\n",
    "    else:\n",
    "        raise ValueError\n",
    "        \n",
    "def get_rect_from_chunk_boundary(chunk_boundary):\n",
    "    \"\"\"given a chunk boundary tuple, return a numpy\n",
    "    array that can be added as a shape to napari\"\n",
    "    From DaskFusion (https://github.com/VolkerH/DaskFusion/)\n",
    "    \"\"\"\n",
    "    ylim, xlim = chunk_boundary\n",
    "    miny, maxy = ylim[0], ylim[1] - 1\n",
    "    minx, maxx = xlim[0], xlim[1] - 1\n",
    "    return np.array([[miny, minx], [maxy, minx], [maxy, maxx], [miny, maxx]])\n",
    "\n",
    "def find_chunk_tile_intersections(\n",
    "    tiles_shapely: List[\"shapely.geometry.base.BaseGeometry\"],\n",
    "    chunks_shapely: List[\"shapely.geometry.base.BaseGeometry\"],\n",
    ") -> Dict[Tuple[int, int], Tuple[str, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    For each output array chunk, find the intersecting image tiles\n",
    "\n",
    "    Args:\n",
    "        tile_shapes: Contains the shapely objects corresponding to transformed image outlines.\n",
    "                    Each shape in tile_shapes must have a .fuse_info dictionary with\n",
    "                    keys \"file\" and \"transform\".\n",
    "        chunk_shapes: Contains the shapely objects representing dask array chunks.\n",
    "                    Each shape in chunk_shapes must have a .fuse_info dictionary with\n",
    "                    key \"chunk_boundary\", containing a tuple of chunk boundaries\n",
    "\n",
    "    Returns:\n",
    "         The chunk_to_tiles dictionary, which has the chunk anchor points as keys and tuples of\n",
    "         image file names and their corresponding affine transform matrix as values.\n",
    "    From DaskFusion (https://github.com/VolkerH/DaskFusion/)\n",
    "    \"\"\"\n",
    "    chunk_to_tiles = {}\n",
    "    tile_tree = STRtree(tiles_shapely)\n",
    "\n",
    "    for chunk_shape in chunks_shapely:\n",
    "        chunk_boundary = chunk_shape.fuse_info[\"chunk_boundary\"]\n",
    "        anchor_point = (chunk_boundary[0][0], chunk_boundary[1][0])\n",
    "        intersecting_tiles = tile_tree.query(chunk_shape)\n",
    "        chunk_to_tiles[anchor_point] = [\n",
    "            ((t.fuse_info[\"file\"], t.fuse_info[\"transform\"]))\n",
    "            for t in intersecting_tiles\n",
    "        ]\n",
    "    return chunk_to_tiles\n",
    "\n",
    "def fuse_func(\n",
    "    input_tile_info: Dict[\n",
    "        Tuple[int, int], List[Tuple[Union[str, Path, np.ndarray], np.ndarray]]\n",
    "    ],\n",
    "    imload_fn: Optional[Callable] = imread,\n",
    "    block_info=None,\n",
    "    dtype=np.uint16,\n",
    ") -> np.ndarray:\n",
    "\n",
    "    \"\"\"\n",
    "    Fuses the tiles that intersect the current chunk of a dask array using maximum projection.\n",
    "\n",
    "    Pass this function to dask.array.map_blocks, after partial evaluation of the required\n",
    "    image_folder and (if needed) optional arguments.\n",
    "\n",
    "    Returns:\n",
    "        Array of chunk-shape containing max projection of tiles falling into chunk\n",
    "        \n",
    "    From DaskFusion (https://github.com/VolkerH/DaskFusion/)\n",
    "    \"\"\"\n",
    "    array_location = block_info[None][\"array-location\"]\n",
    "    # The anchor point is the key to the input_tile_info dictionary\n",
    "    anchor_point = (array_location[0][0], array_location[1][0])\n",
    "    chunk_shape = block_info[None][\"chunk-shape\"]\n",
    "    tiles_info = input_tile_info[anchor_point]\n",
    "    print(f\"Processing chunk at {anchor_point}\")\n",
    "    fused = np.zeros(chunk_shape, dtype=dtype)\n",
    "    for image_representation, tile_affine in tiles_info:\n",
    "        if imload_fn is not None:\n",
    "            # When imload_fn is provided we assume we have been given strings representing files\n",
    "            tile_path = image_representation\n",
    "            im = imload_fn(tile_path)\n",
    "        else:\n",
    "            # Without imload function we assume images are passed\n",
    "            im = image_representation\n",
    "        shift = AffineTransform(translation=(-anchor_point[0], -anchor_point[1]))\n",
    "        tile_shifted = affine_transform(\n",
    "            im,\n",
    "            matrix=np.linalg.inv(shift.params @ tile_affine),\n",
    "            output_shape=chunk_shape,\n",
    "            cval=0,\n",
    "        )\n",
    "        # note that the dtype comversion here happens without scaling\n",
    "        # may want to use one of the skimage.img_as_* functions instead\n",
    "        stack = np.stack([fused, tile_shifted.astype(dtype)])\n",
    "        fused = np.max(stack, axis=0)\n",
    "    return fused\n",
    "\n",
    "FilePath = Union[Path, str]\n",
    "ArrayLike = Union[\n",
    "    np.ndarray, \"dask.array.Array\"\n",
    "]  # could add other array types if needed\n",
    "\n",
    "def load_image(\n",
    "    file: FilePath, transforms: List[Callable[[ArrayLike], ArrayLike]] = None\n",
    ") -> np.ndarray:\n",
    "    img = imread(file)\n",
    "    # if img.ndim == 2:\n",
    "    #    img = np.expand_dims(img, axis=0)\n",
    "    if transforms is not None:\n",
    "        for t in transforms:\n",
    "            img = t(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f5ee5d",
   "metadata": {},
   "source": [
    "#### Find images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60602fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113400 image files found\n"
     ]
    }
   ],
   "source": [
    "image_dir = '/mnt/DATA/sandbox/pierre_live_cell_data/outputs/Replication_IPSDM_GFP/Images/'\n",
    "# image_dir = '/mnt/DATA/sandbox/pierre_live_cell_data/outputs/Replication_IPSDM_GFP/r03c03f01p01/'\n",
    "fns = glob.glob(os.path.join(image_dir, '*.tiff'))\n",
    "# fns = glob.glob(os.path.join(image_dir, f'r0{row}c0{col}*.tiff'))\n",
    "print(len(fns), 'image files found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bfc27c",
   "metadata": {},
   "source": [
    "### Loading metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b40b11ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4664875ad16b46bca081fc8f216a1d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metadata_fn = '/mnt/DATA/sandbox/pierre_live_cell_data/outputs/Replication_IPSDM_GFP/Index.idx.xml'\n",
    "df = read_harmony_metadata(metadata_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "91c34bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>State</th>\n",
       "      <th>URL</th>\n",
       "      <th>Row</th>\n",
       "      <th>Col</th>\n",
       "      <th>FieldID</th>\n",
       "      <th>PlaneID</th>\n",
       "      <th>TimepointID</th>\n",
       "      <th>ChannelID</th>\n",
       "      <th>FlimID</th>\n",
       "      <th>...</th>\n",
       "      <th>PositionZ</th>\n",
       "      <th>AbsPositionZ</th>\n",
       "      <th>MeasurementTimeOffset</th>\n",
       "      <th>AbsTime</th>\n",
       "      <th>MainExcitationWavelength</th>\n",
       "      <th>MainEmissionWavelength</th>\n",
       "      <th>ObjectiveMagnification</th>\n",
       "      <th>ObjectiveNA</th>\n",
       "      <th>ExposureTime</th>\n",
       "      <th>OrientationMatrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0303K1F1P1R1</td>\n",
       "      <td>Ok</td>\n",
       "      <td>r03c03f01p01-ch1sk1fk1fl1.tiff</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135583505</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-04-16T19:09:33.84+01:00</td>\n",
       "      <td>488</td>\n",
       "      <td>522</td>\n",
       "      <td>40</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[[0.990860,0,0,-15.9],[0,-0.990860,0,-44.8],[0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0303K1F1P1R2</td>\n",
       "      <td>Ok</td>\n",
       "      <td>r03c03f01p01-ch2sk1fk1fl1.tiff</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135583505</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-04-16T19:09:33.84+01:00</td>\n",
       "      <td>640</td>\n",
       "      <td>706</td>\n",
       "      <td>40</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>[[0.990860,0,0,-15.9],[0,-0.990860,0,-44.8],[0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0303K1F1P2R1</td>\n",
       "      <td>Ok</td>\n",
       "      <td>r03c03f01p02-ch1sk1fk1fl1.tiff</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2E-06</td>\n",
       "      <td>0.135585502</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-04-16T19:09:34.12+01:00</td>\n",
       "      <td>488</td>\n",
       "      <td>522</td>\n",
       "      <td>40</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[[0.990860,0,0,-15.9],[0,-0.990860,0,-44.8],[0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0303K1F1P2R2</td>\n",
       "      <td>Ok</td>\n",
       "      <td>r03c03f01p02-ch2sk1fk1fl1.tiff</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2E-06</td>\n",
       "      <td>0.135585502</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-04-16T19:09:34.12+01:00</td>\n",
       "      <td>640</td>\n",
       "      <td>706</td>\n",
       "      <td>40</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>[[0.990860,0,0,-15.9],[0,-0.990860,0,-44.8],[0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0303K1F1P3R1</td>\n",
       "      <td>Ok</td>\n",
       "      <td>r03c03f01p03-ch1sk1fk1fl1.tiff</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4E-06</td>\n",
       "      <td>0.135587499</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-04-16T19:09:34.4+01:00</td>\n",
       "      <td>488</td>\n",
       "      <td>522</td>\n",
       "      <td>40</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[[0.990860,0,0,-15.9],[0,-0.990860,0,-44.8],[0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113395</th>\n",
       "      <td>0609K75F9P1R2</td>\n",
       "      <td>Ok</td>\n",
       "      <td>r06c09f09p01-ch2sk75fk1fl1.tiff</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135533601</td>\n",
       "      <td>266399.61</td>\n",
       "      <td>2021-04-19T21:14:19.477+01:00</td>\n",
       "      <td>640</td>\n",
       "      <td>706</td>\n",
       "      <td>40</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>[[0.990860,0,0,-15.9],[0,-0.990860,0,-44.8],[0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113396</th>\n",
       "      <td>0609K75F9P2R1</td>\n",
       "      <td>Ok</td>\n",
       "      <td>r06c09f09p02-ch1sk75fk1fl1.tiff</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2E-06</td>\n",
       "      <td>0.135535598</td>\n",
       "      <td>266399.61</td>\n",
       "      <td>2021-04-19T21:14:19.757+01:00</td>\n",
       "      <td>488</td>\n",
       "      <td>522</td>\n",
       "      <td>40</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[[0.990860,0,0,-15.9],[0,-0.990860,0,-44.8],[0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113397</th>\n",
       "      <td>0609K75F9P2R2</td>\n",
       "      <td>Ok</td>\n",
       "      <td>r06c09f09p02-ch2sk75fk1fl1.tiff</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2E-06</td>\n",
       "      <td>0.135535598</td>\n",
       "      <td>266399.61</td>\n",
       "      <td>2021-04-19T21:14:19.757+01:00</td>\n",
       "      <td>640</td>\n",
       "      <td>706</td>\n",
       "      <td>40</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>[[0.990860,0,0,-15.9],[0,-0.990860,0,-44.8],[0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113398</th>\n",
       "      <td>0609K75F9P3R1</td>\n",
       "      <td>Ok</td>\n",
       "      <td>r06c09f09p03-ch1sk75fk1fl1.tiff</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4E-06</td>\n",
       "      <td>0.135537595</td>\n",
       "      <td>266399.61</td>\n",
       "      <td>2021-04-19T21:14:20.037+01:00</td>\n",
       "      <td>488</td>\n",
       "      <td>522</td>\n",
       "      <td>40</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[[0.990860,0,0,-15.9],[0,-0.990860,0,-44.8],[0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113399</th>\n",
       "      <td>0609K75F9P3R2</td>\n",
       "      <td>Ok</td>\n",
       "      <td>r06c09f09p03-ch2sk75fk1fl1.tiff</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4E-06</td>\n",
       "      <td>0.135537595</td>\n",
       "      <td>266399.61</td>\n",
       "      <td>2021-04-19T21:14:20.037+01:00</td>\n",
       "      <td>640</td>\n",
       "      <td>706</td>\n",
       "      <td>40</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>[[0.990860,0,0,-15.9],[0,-0.990860,0,-44.8],[0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113400 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id State                              URL Row Col FieldID  \\\n",
       "0        0303K1F1P1R1    Ok   r03c03f01p01-ch1sk1fk1fl1.tiff   3   3       1   \n",
       "1        0303K1F1P1R2    Ok   r03c03f01p01-ch2sk1fk1fl1.tiff   3   3       1   \n",
       "2        0303K1F1P2R1    Ok   r03c03f01p02-ch1sk1fk1fl1.tiff   3   3       1   \n",
       "3        0303K1F1P2R2    Ok   r03c03f01p02-ch2sk1fk1fl1.tiff   3   3       1   \n",
       "4        0303K1F1P3R1    Ok   r03c03f01p03-ch1sk1fk1fl1.tiff   3   3       1   \n",
       "...               ...   ...                              ...  ..  ..     ...   \n",
       "113395  0609K75F9P1R2    Ok  r06c09f09p01-ch2sk75fk1fl1.tiff   6   9       9   \n",
       "113396  0609K75F9P2R1    Ok  r06c09f09p02-ch1sk75fk1fl1.tiff   6   9       9   \n",
       "113397  0609K75F9P2R2    Ok  r06c09f09p02-ch2sk75fk1fl1.tiff   6   9       9   \n",
       "113398  0609K75F9P3R1    Ok  r06c09f09p03-ch1sk75fk1fl1.tiff   6   9       9   \n",
       "113399  0609K75F9P3R2    Ok  r06c09f09p03-ch2sk75fk1fl1.tiff   6   9       9   \n",
       "\n",
       "       PlaneID TimepointID ChannelID FlimID  ... PositionZ AbsPositionZ  \\\n",
       "0            1           0         1      1  ...         0  0.135583505   \n",
       "1            1           0         2      1  ...         0  0.135583505   \n",
       "2            2           0         1      1  ...     2E-06  0.135585502   \n",
       "3            2           0         2      1  ...     2E-06  0.135585502   \n",
       "4            3           0         1      1  ...     4E-06  0.135587499   \n",
       "...        ...         ...       ...    ...  ...       ...          ...   \n",
       "113395       1          74         2      1  ...         0  0.135533601   \n",
       "113396       2          74         1      1  ...     2E-06  0.135535598   \n",
       "113397       2          74         2      1  ...     2E-06  0.135535598   \n",
       "113398       3          74         1      1  ...     4E-06  0.135537595   \n",
       "113399       3          74         2      1  ...     4E-06  0.135537595   \n",
       "\n",
       "       MeasurementTimeOffset                        AbsTime  \\\n",
       "0                          0   2021-04-16T19:09:33.84+01:00   \n",
       "1                          0   2021-04-16T19:09:33.84+01:00   \n",
       "2                          0   2021-04-16T19:09:34.12+01:00   \n",
       "3                          0   2021-04-16T19:09:34.12+01:00   \n",
       "4                          0    2021-04-16T19:09:34.4+01:00   \n",
       "...                      ...                            ...   \n",
       "113395             266399.61  2021-04-19T21:14:19.477+01:00   \n",
       "113396             266399.61  2021-04-19T21:14:19.757+01:00   \n",
       "113397             266399.61  2021-04-19T21:14:19.757+01:00   \n",
       "113398             266399.61  2021-04-19T21:14:20.037+01:00   \n",
       "113399             266399.61  2021-04-19T21:14:20.037+01:00   \n",
       "\n",
       "       MainExcitationWavelength MainEmissionWavelength ObjectiveMagnification  \\\n",
       "0                           488                    522                     40   \n",
       "1                           640                    706                     40   \n",
       "2                           488                    522                     40   \n",
       "3                           640                    706                     40   \n",
       "4                           488                    522                     40   \n",
       "...                         ...                    ...                    ...   \n",
       "113395                      640                    706                     40   \n",
       "113396                      488                    522                     40   \n",
       "113397                      640                    706                     40   \n",
       "113398                      488                    522                     40   \n",
       "113399                      640                    706                     40   \n",
       "\n",
       "       ObjectiveNA ExposureTime  \\\n",
       "0              1.1          0.1   \n",
       "1              1.1          0.2   \n",
       "2              1.1          0.1   \n",
       "3              1.1          0.2   \n",
       "4              1.1          0.1   \n",
       "...            ...          ...   \n",
       "113395         1.1          0.2   \n",
       "113396         1.1          0.1   \n",
       "113397         1.1          0.2   \n",
       "113398         1.1          0.1   \n",
       "113399         1.1          0.2   \n",
       "\n",
       "                                        OrientationMatrix  \n",
       "0       [[0.990860,0,0,-15.9],[0,-0.990860,0,-44.8],[0...  \n",
       "1       [[0.990860,0,0,-15.9],[0,-0.990860,0,-44.8],[0...  \n",
       "2       [[0.990860,0,0,-15.9],[0,-0.990860,0,-44.8],[0...  \n",
       "3       [[0.990860,0,0,-15.9],[0,-0.990860,0,-44.8],[0...  \n",
       "4       [[0.990860,0,0,-15.9],[0,-0.990860,0,-44.8],[0...  \n",
       "...                                                   ...  \n",
       "113395  [[0.990860,0,0,-15.9],[0,-0.990860,0,-44.8],[0...  \n",
       "113396  [[0.990860,0,0,-15.9],[0,-0.990860,0,-44.8],[0...  \n",
       "113397  [[0.990860,0,0,-15.9],[0,-0.990860,0,-44.8],[0...  \n",
       "113398  [[0.990860,0,0,-15.9],[0,-0.990860,0,-44.8],[0...  \n",
       "113399  [[0.990860,0,0,-15.9],[0,-0.990860,0,-44.8],[0...  \n",
       "\n",
       "[113400 rows x 35 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d078708",
   "metadata": {},
   "source": [
    "#### Get position information from metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c759b207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051203a898074508b4ed2949f1f8d187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position index and (row,column): 0 (3, 4)\n",
      "Position index and (row,column): 1 (4, 3)\n",
      "Position index and (row,column): 2 (4, 9)\n",
      "Position index and (row,column): 3 (3, 7)\n",
      "Position index and (row,column): 4 (5, 4)\n",
      "Position index and (row,column): 5 (4, 6)\n",
      "Position index and (row,column): 6 (3, 10)\n",
      "Position index and (row,column): 7 (5, 7)\n",
      "Position index and (row,column): 8 (6, 5)\n",
      "Position index and (row,column): 9 (6, 8)\n",
      "Position index and (row,column): 10 (4, 5)\n",
      "Position index and (row,column): 11 (3, 3)\n",
      "Position index and (row,column): 12 (3, 9)\n",
      "Position index and (row,column): 13 (5, 6)\n",
      "Position index and (row,column): 14 (4, 8)\n",
      "Position index and (row,column): 15 (3, 6)\n",
      "Position index and (row,column): 16 (5, 9)\n",
      "Position index and (row,column): 17 (6, 4)\n",
      "Position index and (row,column): 18 (6, 7)\n",
      "Position index and (row,column): 19 (4, 7)\n",
      "Position index and (row,column): 20 (3, 5)\n",
      "Position index and (row,column): 21 (4, 4)\n",
      "Position index and (row,column): 22 (4, 10)\n",
      "Position index and (row,column): 23 (3, 8)\n",
      "Position index and (row,column): 24 (5, 5)\n",
      "Position index and (row,column): 25 (5, 8)\n",
      "Position index and (row,column): 26 (6, 6)\n",
      "Position index and (row,column): 27 (6, 9)\n"
     ]
    }
   ],
   "source": [
    "row_col_list = list()\n",
    "for index, row in tqdm(df.iterrows(), total = len(df)):\n",
    "    row_col_list.append(tuple((int(row['Row']), int(row['Col']))))\n",
    "row_col_list = list(set(row_col_list))\n",
    "for n, i in enumerate(row_col_list):\n",
    "    print('Position index and (row,column):', n, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb986e8",
   "metadata": {},
   "source": [
    "## Get dimensionality of image volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff0f0243",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_IDs = df['ChannelID'].unique()\n",
    "plane_IDs = df['PlaneID'].unique()\n",
    "timepoint_IDs = df['TimepointID'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e5f28c",
   "metadata": {},
   "source": [
    "## Set mosaic parameters\n",
    "\n",
    "The `chunk_fraction` is how many sections you want one slice cut up into (has to be a square) and the `_load_image` partial function can include any image transformations you wish (border crop or background removal etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "decc841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_fraction = 36\n",
    "_load_image = partial(load_image, transforms=[])#input_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efea1beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1ac003a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_fuse_func=partial(fuse_func, \n",
    "                   imload_fn=_load_image,\n",
    "                   dtype=imread(fns[0]).dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aa9a1672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.1 ms ± 6.73 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "skimage.io.imread(fns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "be6024a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2160, 2160)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skim = skimage.io.imread(fns[0])\n",
    "skim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2cbf8e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "423babe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14 ms ± 171 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "dask_image.imread.imread(fns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "da3e604c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2160, 2160)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimage = dask_image.imread.imread(fns[0])\n",
    "dimage[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "55d1ee89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 8.90 MiB </td> <td> 8.90 MiB </td></tr>\n",
       "    <tr><th> Shape </th><td> (1, 2160, 2160) </td> <td> (1, 2160, 2160) </td></tr>\n",
       "    <tr><th> Count </th><td> 3 Tasks </td><td> 1 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> uint16 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"194\" height=\"184\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"24\" y2=\"14\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"10\" y1=\"120\" x2=\"24\" y2=\"134\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"24\" y1=\"14\" x2=\"24\" y2=\"134\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 24.9485979497544,14.948597949754403 24.9485979497544,134.9485979497544 10.0,120.0\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"130\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"24\" y1=\"14\" x2=\"144\" y2=\"14\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"24\" y2=\"14\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"130\" y1=\"0\" x2=\"144\" y2=\"14\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 130.0,0.0 144.9485979497544,14.948597949754403 24.9485979497544,14.948597949754403\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"24\" y1=\"14\" x2=\"144\" y2=\"14\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"24\" y1=\"134\" x2=\"144\" y2=\"134\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"24\" y1=\"14\" x2=\"24\" y2=\"134\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"144\" y1=\"14\" x2=\"144\" y2=\"134\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"24.9485979497544,14.948597949754403 144.9485979497544,14.948597949754403 144.9485979497544,134.9485979497544 24.9485979497544,134.9485979497544\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"84.948598\" y=\"154.948598\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >2160</text>\n",
       "  <text x=\"164.948598\" y=\"74.948598\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,164.948598,74.948598)\">2160</text>\n",
       "  <text x=\"7.474299\" y=\"147.474299\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,7.474299,147.474299)\">1</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<_map_read_frame, shape=(1, 2160, 2160), dtype=uint16, chunksize=(1, 2160, 2160), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ac37de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = skim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eae58822",
   "metadata": {},
   "outputs": [],
   "source": [
    "if img.ndim == 2:\n",
    "       img = np.expand_dims(img, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fae1c76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2160, 2160)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c0538c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(\n",
    "    file: FilePath, transforms: List[Callable[[ArrayLike], ArrayLike]] = None\n",
    ") -> np.ndarray:\n",
    "    img = imread(file)[0]\n",
    "#     if img.ndim == 2:\n",
    "#         img = np.expand_dims(img, axis=0)\n",
    "    if transforms is not None:\n",
    "        for t in transforms:\n",
    "            img = t(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "58ccd666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "90278c97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28edeaa89f3a49dc89a8bbc4a583352d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = dict()\n",
    "for row, col in tqdm(row_col_list):\n",
    "    ### define row and col\n",
    "    row, col = str(row), str(col)\n",
    "    ### clear empty arrays for organsing into dask arrays\n",
    "    t_stack = []\n",
    "    ### iterate over each individual image slice, tiling together\n",
    "    for time in (timepoint_IDs):\n",
    "        c_stack = []\n",
    "        for channel in (channel_IDs):\n",
    "            z_stack = []\n",
    "            for plane in (plane_IDs):\n",
    "                ### extract metadata for this mosaic\n",
    "                filtered_df = df[(df['TimepointID'] == time)  \n",
    "                               &(df['PlaneID'] == plane)\n",
    "                               &(df['ChannelID'] == channel)\n",
    "                               &(df['Row'] == row) \n",
    "                               &(df['Col'] == col)\n",
    "                                ]\n",
    "                ### extract filenames for subset\n",
    "                fns = filtered_df['URL']\n",
    "                ### build into full file path\n",
    "                fns = [glob.glob(os.path.join(image_dir, fn))[0] for fn in fns]\n",
    "                ### stack single slice mosaic into lazy array\n",
    "                sample = skimage.io.imread(fns[0])\n",
    "                lazy_arrays = [dask.delayed(skimage.io.imread)(fn) for fn in fns]\n",
    "                lazy_arrays = [da.from_delayed(x, shape=sample.shape, dtype=sample.dtype)#'uint8')#\n",
    "                               for x in lazy_arrays]\n",
    "                #print(len(lazy_arrays), 'images lazily loaded')\n",
    "                ### extract and convert coordinates from standard units into pixels\n",
    "                coords = filtered_df[[\"URL\", \"PositionX\", \"PositionY\", \"PositionZ\", \"ImageResolutionX\", \"ImageResolutionY\"]]\n",
    "                coords['PositionXPix'] = (coords['PositionX'].astype(float))/(coords['ImageResolutionX']).astype(float)\n",
    "                coords['PositionYPix'] = (coords['PositionY'].astype(float))/(coords['ImageResolutionY']).astype(float)\n",
    "                norm_coords = list(zip(coords['PositionXPix'], coords['PositionYPix']))\n",
    "                ### convert tile coordinates into transformation matrices\n",
    "                transforms = [AffineTransform(translation=stage_coord).params for stage_coord in norm_coords]\n",
    "                tiles = [transform_tile_coord(sample.shape, transform) for transform in transforms]\n",
    "                ### shift the tile coordinates to the origin\n",
    "                all_bboxes = np.vstack(tiles)\n",
    "                all_min = all_bboxes.min(axis=0)\n",
    "                all_max = all_bboxes.max(axis=0)\n",
    "                stitched_shape=tuple(np.ceil(all_max-all_min).astype(int))\n",
    "                shift_to_origin = AffineTransform(translation=-all_min)\n",
    "                transforms_with_shift = [t @ shift_to_origin.params for t in transforms]\n",
    "                shifted_tiles = [transform_tile_coord(sample.shape, t) for t in transforms_with_shift]\n",
    "                ### decide on chunk size as a fraction of total slice size \n",
    "                chunk_size = (6048/np.sqrt(chunk_fraction),6048/np.sqrt(chunk_fraction))\n",
    "                chunks = normalize_chunks(chunk_size,shape=tuple(stitched_shape))\n",
    "                ### check the maths adds up correctly (chunks fit into mosaic)\n",
    "                computed_shape = np.array(list(map(sum, chunks)))\n",
    "                assert np.all(np.array(stitched_shape) == computed_shape)\n",
    "                ### get boundary coords of chunks\n",
    "                chunk_boundaries = list(get_chunk_coord(stitched_shape, chunk_size))\n",
    "                ### use shapely to find the intersection of the chunks\n",
    "                tiles_shifted_shapely = [numpy_shape_to_shapely(s) for s in shifted_tiles]\n",
    "                chunk_shapes = list(map(get_rect_from_chunk_boundary, chunk_boundaries))\n",
    "                chunks_shapely = [numpy_shape_to_shapely(c) for c in chunk_shapes]\n",
    "                ### build dictionary of chunk shape data with filenames and transformations\n",
    "                for tile_shifted_shapely, file, transform in zip(tiles_shifted_shapely, \n",
    "                                                     fns, \n",
    "                                                     transforms_with_shift):\n",
    "                    tile_shifted_shapely.fuse_info = {'file':file, \n",
    "                                                      'transform':transform}\n",
    "                for chunk_shapely, chunk_boundary in zip(chunks_shapely, \n",
    "                                                          chunk_boundaries):\n",
    "                    chunk_shapely.fuse_info = {'chunk_boundary': chunk_boundary}\n",
    "                chunk_tiles = find_chunk_tile_intersections(tiles_shifted_shapely, chunks_shapely)\n",
    "                ### tile images together                \n",
    "                frame = da.map_blocks(func=_fuse_func,\n",
    "                         chunks=chunks, \n",
    "                         input_tile_info=chunk_tiles,\n",
    "                         dtype=sample.dtype)\n",
    "                ### collect stitched frames together into time stack\n",
    "                z_stack.append(frame)\n",
    "            ### stack time series together in z\n",
    "            c_stack.append(z_stack)\n",
    "        ### stack together channelwise\n",
    "        t_stack.append(c_stack)\n",
    "    ### stack stitched dask arrays together into multidim image volumes\n",
    "    images[(int(row), int(col))] = da.stack([da.stack(c_stack, axis = 0) for c_stack in t_stack])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1857d81",
   "metadata": {},
   "source": [
    "## I am already mapping the blocks together inside the stitching so just need to stack in the correct way now - if i change the order of iteration could i stack together in the right way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2e870734",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = images[(3,4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e5265c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 2, 3, 6048, 6048)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[(3,4)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "838e8209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 30.66 GiB </td> <td> 1.94 MiB </td></tr>\n",
       "    <tr><th> Shape </th><td> (75, 2, 3, 6048, 6048) </td> <td> (1, 1, 1, 1008, 1008) </td></tr>\n",
       "    <tr><th> Count </th><td> 81000 Tasks </td><td> 16200 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> uint16 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"374\" height=\"184\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" />\n",
       "  <line x1=\"0\" y1=\"1\" x2=\"25\" y2=\"1\" />\n",
       "  <line x1=\"0\" y1=\"2\" x2=\"25\" y2=\"2\" />\n",
       "  <line x1=\"0\" y1=\"3\" x2=\"25\" y2=\"3\" />\n",
       "  <line x1=\"0\" y1=\"3\" x2=\"25\" y2=\"3\" />\n",
       "  <line x1=\"0\" y1=\"4\" x2=\"25\" y2=\"4\" />\n",
       "  <line x1=\"0\" y1=\"5\" x2=\"25\" y2=\"5\" />\n",
       "  <line x1=\"0\" y1=\"6\" x2=\"25\" y2=\"6\" />\n",
       "  <line x1=\"0\" y1=\"7\" x2=\"25\" y2=\"7\" />\n",
       "  <line x1=\"0\" y1=\"8\" x2=\"25\" y2=\"8\" />\n",
       "  <line x1=\"0\" y1=\"8\" x2=\"25\" y2=\"8\" />\n",
       "  <line x1=\"0\" y1=\"9\" x2=\"25\" y2=\"9\" />\n",
       "  <line x1=\"0\" y1=\"10\" x2=\"25\" y2=\"10\" />\n",
       "  <line x1=\"0\" y1=\"11\" x2=\"25\" y2=\"11\" />\n",
       "  <line x1=\"0\" y1=\"12\" x2=\"25\" y2=\"12\" />\n",
       "  <line x1=\"0\" y1=\"13\" x2=\"25\" y2=\"13\" />\n",
       "  <line x1=\"0\" y1=\"13\" x2=\"25\" y2=\"13\" />\n",
       "  <line x1=\"0\" y1=\"14\" x2=\"25\" y2=\"14\" />\n",
       "  <line x1=\"0\" y1=\"15\" x2=\"25\" y2=\"15\" />\n",
       "  <line x1=\"0\" y1=\"16\" x2=\"25\" y2=\"16\" />\n",
       "  <line x1=\"0\" y1=\"17\" x2=\"25\" y2=\"17\" />\n",
       "  <line x1=\"0\" y1=\"18\" x2=\"25\" y2=\"18\" />\n",
       "  <line x1=\"0\" y1=\"18\" x2=\"25\" y2=\"18\" />\n",
       "  <line x1=\"0\" y1=\"19\" x2=\"25\" y2=\"19\" />\n",
       "  <line x1=\"0\" y1=\"20\" x2=\"25\" y2=\"20\" />\n",
       "  <line x1=\"0\" y1=\"21\" x2=\"25\" y2=\"21\" />\n",
       "  <line x1=\"0\" y1=\"22\" x2=\"25\" y2=\"22\" />\n",
       "  <line x1=\"0\" y1=\"22\" x2=\"25\" y2=\"22\" />\n",
       "  <line x1=\"0\" y1=\"23\" x2=\"25\" y2=\"23\" />\n",
       "  <line x1=\"0\" y1=\"24\" x2=\"25\" y2=\"24\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"25\" y2=\"25\" />\n",
       "  <line x1=\"0\" y1=\"26\" x2=\"25\" y2=\"26\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"26\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"12\" y1=\"0\" x2=\"12\" y2=\"26\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"26\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 25.412616514582485,0.0 25.412616514582485,26.472362419580787 0.0,26.472362419580787\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"46.472362\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >2</text>\n",
       "  <text x=\"45.412617\" y=\"13.236181\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,45.412617,13.236181)\">75</text>\n",
       "\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"95\" y1=\"0\" x2=\"109\" y2=\"14\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"95\" y1=\"20\" x2=\"109\" y2=\"34\" />\n",
       "  <line x1=\"95\" y1=\"40\" x2=\"109\" y2=\"54\" />\n",
       "  <line x1=\"95\" y1=\"60\" x2=\"109\" y2=\"74\" />\n",
       "  <line x1=\"95\" y1=\"80\" x2=\"109\" y2=\"94\" />\n",
       "  <line x1=\"95\" y1=\"100\" x2=\"109\" y2=\"114\" />\n",
       "  <line x1=\"95\" y1=\"120\" x2=\"109\" y2=\"134\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"95\" y1=\"0\" x2=\"95\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"99\" y1=\"4\" x2=\"99\" y2=\"124\" />\n",
       "  <line x1=\"104\" y1=\"9\" x2=\"104\" y2=\"129\" />\n",
       "  <line x1=\"109\" y1=\"14\" x2=\"109\" y2=\"134\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"95.0,0.0 109.9485979497544,14.948597949754403 109.9485979497544,134.9485979497544 95.0,120.0\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"95\" y1=\"0\" x2=\"215\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"99\" y1=\"4\" x2=\"219\" y2=\"4\" />\n",
       "  <line x1=\"104\" y1=\"9\" x2=\"224\" y2=\"9\" />\n",
       "  <line x1=\"109\" y1=\"14\" x2=\"229\" y2=\"14\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"95\" y1=\"0\" x2=\"109\" y2=\"14\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"115\" y1=\"0\" x2=\"129\" y2=\"14\" />\n",
       "  <line x1=\"135\" y1=\"0\" x2=\"149\" y2=\"14\" />\n",
       "  <line x1=\"155\" y1=\"0\" x2=\"169\" y2=\"14\" />\n",
       "  <line x1=\"175\" y1=\"0\" x2=\"189\" y2=\"14\" />\n",
       "  <line x1=\"195\" y1=\"0\" x2=\"209\" y2=\"14\" />\n",
       "  <line x1=\"215\" y1=\"0\" x2=\"229\" y2=\"14\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"95.0,0.0 215.0,0.0 229.9485979497544,14.948597949754403 109.9485979497544,14.948597949754403\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"109\" y1=\"14\" x2=\"229\" y2=\"14\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"109\" y1=\"34\" x2=\"229\" y2=\"34\" />\n",
       "  <line x1=\"109\" y1=\"54\" x2=\"229\" y2=\"54\" />\n",
       "  <line x1=\"109\" y1=\"74\" x2=\"229\" y2=\"74\" />\n",
       "  <line x1=\"109\" y1=\"94\" x2=\"229\" y2=\"94\" />\n",
       "  <line x1=\"109\" y1=\"114\" x2=\"229\" y2=\"114\" />\n",
       "  <line x1=\"109\" y1=\"134\" x2=\"229\" y2=\"134\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"109\" y1=\"14\" x2=\"109\" y2=\"134\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"129\" y1=\"14\" x2=\"129\" y2=\"134\" />\n",
       "  <line x1=\"149\" y1=\"14\" x2=\"149\" y2=\"134\" />\n",
       "  <line x1=\"169\" y1=\"14\" x2=\"169\" y2=\"134\" />\n",
       "  <line x1=\"189\" y1=\"14\" x2=\"189\" y2=\"134\" />\n",
       "  <line x1=\"209\" y1=\"14\" x2=\"209\" y2=\"134\" />\n",
       "  <line x1=\"229\" y1=\"14\" x2=\"229\" y2=\"134\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"109.9485979497544,14.948597949754403 229.9485979497544,14.948597949754403 229.9485979497544,134.9485979497544 109.9485979497544,134.9485979497544\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"169.948598\" y=\"154.948598\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >6048</text>\n",
       "  <text x=\"249.948598\" y=\"74.948598\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,249.948598,74.948598)\">6048</text>\n",
       "  <text x=\"92.474299\" y=\"147.474299\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,92.474299,147.474299)\">3</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<stack, shape=(75, 2, 3, 6048, 6048), dtype=uint16, chunksize=(1, 1, 1, 1008, 1008), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk at (0, 0)\n",
      "Processing chunk at (0, 1008)\n",
      "Processing chunk at (0, 2016)\n",
      "Processing chunk at (0, 3024)\n",
      "Processing chunk at (0, 4032)\n",
      "Processing chunk at (0, 5040)\n",
      "Processing chunk at (1008, 0)\n",
      "Processing chunk at (1008, 1008)\n",
      "Processing chunk at (1008, 2016)Processing chunk at (1008, 3024)\n",
      "\n",
      "Processing chunk at (1008, 4032)\n",
      "Processing chunk at (1008, 5040)\n",
      "Processing chunk at (2016, 0)Processing chunk at (2016, 1008)\n",
      "Processing chunk at (2016, 2016)\n",
      "\n",
      "Processing chunk at (2016, 3024)\n",
      "Processing chunk at (2016, 4032)\n",
      "Processing chunk at (2016, 5040)\n",
      "Processing chunk at (3024, 0)Processing chunk at (3024, 1008)\n",
      "\n",
      "Processing chunk at (3024, 2016)\n",
      "Processing chunk at (3024, 3024)Processing chunk at (3024, 4032)\n",
      "Processing chunk at (3024, 5040)\n",
      "Processing chunk at (4032, 0)\n",
      "\n",
      "Processing chunk at (4032, 1008)Processing chunk at (4032, 2016)\n",
      "\n",
      "Processing chunk at (4032, 3024)Processing chunk at (4032, 4032)\n",
      "\n",
      "Processing chunk at (4032, 5040)\n",
      "Processing chunk at (5040, 0)Processing chunk at (5040, 1008)\n",
      "Processing chunk at (5040, 2016)\n",
      "\n",
      "Processing chunk at (5040, 3024)\n",
      "Processing chunk at (5040, 4032)Processing chunk at (5040, 5040)\n",
      "\n",
      "Processing chunk at (0, 0)\n",
      "Processing chunk at (0, 1008)\n",
      "Processing chunk at (0, 2016)Processing chunk at (0, 3024)\n",
      "\n",
      "Processing chunk at (0, 4032)\n",
      "Processing chunk at (0, 5040)\n",
      "Processing chunk at (1008, 0)\n",
      "Processing chunk at (1008, 1008)\n",
      "Processing chunk at (1008, 2016)\n",
      "Processing chunk at (1008, 3024)Processing chunk at (1008, 4032)\n",
      "\n",
      "Processing chunk at (1008, 5040)\n",
      "Processing chunk at (2016, 0)\n",
      "Processing chunk at (2016, 1008)\n",
      "Processing chunk at (2016, 2016)\n",
      "Processing chunk at (2016, 3024)\n",
      "Processing chunk at (2016, 4032)Processing chunk at (2016, 5040)\n",
      "\n",
      "Processing chunk at (3024, 0)Processing chunk at (3024, 1008)\n",
      "Processing chunk at (3024, 2016)\n",
      "\n",
      "Processing chunk at (3024, 3024)\n",
      "Processing chunk at (3024, 4032)\n",
      "Processing chunk at (3024, 5040)\n",
      "Processing chunk at (4032, 0)\n",
      "Processing chunk at (4032, 1008)\n",
      "Processing chunk at (4032, 2016)Processing chunk at (4032, 3024)\n",
      "Processing chunk at (4032, 4032)Processing chunk at (4032, 5040)Processing chunk at (5040, 0)\n",
      "\n",
      "\n",
      "Processing chunk at (5040, 1008)\n",
      "\n",
      "Processing chunk at (5040, 2016)Processing chunk at (5040, 3024)\n",
      "Processing chunk at (5040, 4032)Processing chunk at (5040, 5040)\n",
      "\n",
      "\n",
      "Processing chunk at (0, 0)Processing chunk at (0, 1008)\n",
      "\n",
      "Processing chunk at (0, 2016)Processing chunk at (0, 3024)\n",
      "Processing chunk at (0, 4032)\n",
      "Processing chunk at (0, 5040)\n",
      "Processing chunk at (1008, 0)\n",
      "Processing chunk at (1008, 1008)\n",
      "\n",
      "Processing chunk at (1008, 2016)\n",
      "Processing chunk at (1008, 3024)\n",
      "Processing chunk at (1008, 4032)\n",
      "Processing chunk at (1008, 5040)\n",
      "Processing chunk at (2016, 0)Processing chunk at (2016, 1008)\n",
      "Processing chunk at (2016, 2016)\n",
      "\n",
      "Processing chunk at (2016, 3024)Processing chunk at (2016, 4032)\n",
      "\n",
      "Processing chunk at (2016, 5040)\n",
      "Processing chunk at (3024, 0)\n",
      "Processing chunk at (3024, 1008)Processing chunk at (3024, 2016)\n",
      "Processing chunk at (3024, 3024)\n",
      "Processing chunk at (3024, 4032)\n",
      "Processing chunk at (3024, 5040)\n",
      "Processing chunk at (4032, 0)\n",
      "Processing chunk at (4032, 1008)\n",
      "\n",
      "Processing chunk at (4032, 2016)\n",
      "Processing chunk at (4032, 3024)\n",
      "Processing chunk at (4032, 4032)Processing chunk at (4032, 5040)\n",
      "Processing chunk at (5040, 0)Processing chunk at (5040, 1008)\n",
      "Processing chunk at (5040, 2016)\n",
      "\n",
      "\n",
      "Processing chunk at (5040, 3024)Processing chunk at (5040, 4032)\n",
      "Processing chunk at (5040, 5040)\n",
      "\n",
      "Processing chunk at (0, 0)Processing chunk at (0, 1008)\n",
      "Processing chunk at (0, 2016)\n",
      "Processing chunk at (0, 3024)\n",
      "\n",
      "Processing chunk at (0, 4032)\n",
      "Processing chunk at (0, 5040)\n",
      "Processing chunk at (1008, 0)\n",
      "Processing chunk at (1008, 1008)\n",
      "Processing chunk at (1008, 2016)\n",
      "Processing chunk at (1008, 3024)\n",
      "Processing chunk at (1008, 4032)\n",
      "Processing chunk at (1008, 5040)\n",
      "Processing chunk at (2016, 0)\n",
      "Processing chunk at (2016, 1008)\n",
      "Processing chunk at (2016, 2016)\n",
      "Processing chunk at (2016, 3024)\n",
      "Processing chunk at (2016, 4032)Processing chunk at (2016, 5040)\n",
      "Processing chunk at (3024, 0)\n",
      "\n",
      "Processing chunk at (3024, 1008)\n",
      "Processing chunk at (3024, 2016)\n",
      "Processing chunk at (3024, 3024)\n",
      "Processing chunk at (3024, 4032)\n",
      "Processing chunk at (3024, 5040)Processing chunk at (4032, 0)\n",
      "\n",
      "Processing chunk at (4032, 1008)\n",
      "Processing chunk at (4032, 2016)Processing chunk at (4032, 3024)\n",
      "Processing chunk at (4032, 4032)\n",
      "\n",
      "Processing chunk at (4032, 5040)\n",
      "Processing chunk at (5040, 0)Processing chunk at (5040, 1008)\n",
      "\n",
      "Processing chunk at (5040, 2016)\n",
      "Processing chunk at (5040, 3024)Processing chunk at (5040, 4032)\n",
      "\n",
      "Processing chunk at (5040, 5040)\n",
      "Processing chunk at (0, 0)Processing chunk at (0, 1008)\n",
      "\n",
      "Processing chunk at (0, 2016)Processing chunk at (0, 3024)\n",
      "\n",
      "Processing chunk at (0, 4032)Processing chunk at (0, 5040)\n",
      "\n",
      "Processing chunk at (1008, 0)\n",
      "Processing chunk at (1008, 1008)Processing chunk at (1008, 2016)\n",
      "\n",
      "Processing chunk at (1008, 3024)\n",
      "Processing chunk at (1008, 4032)Processing chunk at (1008, 5040)\n",
      "\n",
      "Processing chunk at (2016, 0)Processing chunk at (2016, 1008)\n",
      "Processing chunk at (2016, 2016)\n",
      "Processing chunk at (2016, 3024)\n",
      "\n",
      "Processing chunk at (2016, 4032)\n",
      "Processing chunk at (2016, 5040)Processing chunk at (3024, 0)\n",
      "\n",
      "Processing chunk at (3024, 1008)Processing chunk at (3024, 2016)\n",
      "\n",
      "Processing chunk at (3024, 3024)\n",
      "Processing chunk at (3024, 4032)Processing chunk at (3024, 5040)\n",
      "\n",
      "Processing chunk at (4032, 0)\n",
      "Processing chunk at (4032, 1008)\n",
      "Processing chunk at (4032, 2016)\n",
      "Processing chunk at (4032, 3024)\n",
      "Processing chunk at (4032, 4032)\n",
      "Processing chunk at (4032, 5040)\n",
      "Processing chunk at (5040, 0)Processing chunk at (5040, 1008)\n",
      "\n",
      "Processing chunk at (5040, 2016)Processing chunk at (5040, 3024)\n",
      "\n",
      "Processing chunk at (5040, 4032)\n",
      "Processing chunk at (5040, 5040)\n",
      "Processing chunk at (0, 0)\n",
      "Processing chunk at (0, 1008)\n",
      "Processing chunk at (0, 2016)\n",
      "Processing chunk at (0, 3024)\n",
      "Processing chunk at (0, 4032)\n",
      "Processing chunk at (0, 5040)Processing chunk at (1008, 0)\n",
      "\n",
      "Processing chunk at (1008, 1008)\n",
      "Processing chunk at (1008, 2016)Processing chunk at (1008, 3024)\n",
      "\n",
      "Processing chunk at (1008, 4032)\n",
      "Processing chunk at (1008, 5040)Processing chunk at (2016, 0)\n",
      "\n",
      "Processing chunk at (2016, 1008)\n",
      "Processing chunk at (2016, 2016)Processing chunk at (2016, 3024)\n",
      "Processing chunk at (2016, 4032)\n",
      "Processing chunk at (2016, 5040)\n",
      "\n",
      "Processing chunk at (3024, 0)\n",
      "Processing chunk at (3024, 1008)Processing chunk at (3024, 2016)\n",
      "\n",
      "Processing chunk at (3024, 3024)\n",
      "Processing chunk at (3024, 4032)\n",
      "Processing chunk at (3024, 5040)\n",
      "Processing chunk at (4032, 0)Processing chunk at (4032, 1008)\n",
      "\n",
      "Processing chunk at (4032, 2016)\n",
      "Processing chunk at (4032, 3024)Processing chunk at (4032, 4032)\n",
      "\n",
      "Processing chunk at (4032, 5040)Processing chunk at (5040, 0)\n",
      "\n",
      "Processing chunk at (5040, 1008)Processing chunk at (5040, 2016)\n",
      "Processing chunk at (5040, 3024)\n",
      "Processing chunk at (5040, 4032)\n",
      "Processing chunk at (5040, 5040)\n",
      "\n",
      "Processing chunk at (0, 0)\n",
      "Processing chunk at (0, 1008)\n",
      "Processing chunk at (0, 2016)\n",
      "Processing chunk at (0, 3024)\n",
      "Processing chunk at (0, 4032)\n",
      "Processing chunk at (0, 5040)\n",
      "Processing chunk at (1008, 0)\n",
      "Processing chunk at (1008, 1008)\n",
      "Processing chunk at (1008, 2016)\n",
      "Processing chunk at (1008, 3024)\n",
      "Processing chunk at (1008, 4032)\n",
      "Processing chunk at (1008, 5040)\n",
      "Processing chunk at (2016, 0)\n",
      "Processing chunk at (2016, 1008)\n",
      "Processing chunk at (2016, 2016)\n",
      "Processing chunk at (2016, 3024)\n",
      "Processing chunk at (2016, 4032)\n",
      "Processing chunk at (2016, 5040)\n",
      "Processing chunk at (3024, 0)\n",
      "Processing chunk at (3024, 1008)\n",
      "Processing chunk at (3024, 2016)Processing chunk at (3024, 3024)\n",
      "\n",
      "Processing chunk at (3024, 4032)\n",
      "Processing chunk at (3024, 5040)\n",
      "Processing chunk at (4032, 0)\n",
      "Processing chunk at (4032, 1008)\n",
      "Processing chunk at (4032, 2016)\n",
      "Processing chunk at (4032, 3024)Processing chunk at (4032, 4032)\n",
      "\n",
      "Processing chunk at (4032, 5040)\n",
      "Processing chunk at (5040, 0)\n",
      "Processing chunk at (5040, 1008)Processing chunk at (5040, 2016)\n",
      "Processing chunk at (5040, 3024)\n",
      "\n",
      "Processing chunk at (5040, 4032)\n",
      "Processing chunk at (5040, 5040)\n",
      "Processing chunk at (0, 0)\n",
      "Processing chunk at (0, 1008)\n",
      "Processing chunk at (0, 2016)\n",
      "Processing chunk at (0, 3024)\n",
      "Processing chunk at (0, 4032)\n",
      "Processing chunk at (0, 5040)\n",
      "Processing chunk at (1008, 0)\n",
      "Processing chunk at (1008, 1008)\n",
      "Processing chunk at (1008, 2016)\n",
      "Processing chunk at (1008, 3024)\n",
      "Processing chunk at (1008, 4032)\n",
      "Processing chunk at (1008, 5040)\n",
      "Processing chunk at (2016, 0)\n",
      "Processing chunk at (2016, 1008)\n",
      "Processing chunk at (2016, 2016)\n",
      "Processing chunk at (2016, 3024)\n",
      "Processing chunk at (2016, 4032)Processing chunk at (2016, 5040)\n",
      "\n",
      "Processing chunk at (3024, 0)\n",
      "Processing chunk at (3024, 1008)\n",
      "Processing chunk at (3024, 2016)\n",
      "Processing chunk at (3024, 3024)Processing chunk at (3024, 4032)\n",
      "\n",
      "Processing chunk at (3024, 5040)Processing chunk at (4032, 0)\n",
      "\n",
      "Processing chunk at (4032, 1008)\n",
      "Processing chunk at (4032, 2016)\n",
      "Processing chunk at (4032, 3024)\n",
      "Processing chunk at (4032, 4032)Processing chunk at (4032, 5040)Processing chunk at (5040, 0)\n",
      "Processing chunk at (5040, 1008)\n",
      "\n",
      "Processing chunk at (5040, 2016)\n",
      "Processing chunk at (5040, 3024)\n",
      "Processing chunk at (5040, 4032)\n",
      "Processing chunk at (5040, 5040)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk at (0, 0)\n",
      "Processing chunk at (0, 1008)\n",
      "Processing chunk at (0, 2016)\n",
      "Processing chunk at (0, 3024)\n",
      "Processing chunk at (0, 4032)\n",
      "Processing chunk at (0, 5040)\n",
      "Processing chunk at (1008, 0)\n",
      "Processing chunk at (1008, 1008)Processing chunk at (1008, 2016)\n",
      "Processing chunk at (1008, 3024)\n",
      "\n",
      "Processing chunk at (1008, 4032)Processing chunk at (1008, 5040)\n",
      "\n",
      "Processing chunk at (2016, 0)\n",
      "Processing chunk at (2016, 1008)\n",
      "Processing chunk at (2016, 2016)Processing chunk at (2016, 3024)\n",
      "Processing chunk at (2016, 4032)\n",
      "Processing chunk at (2016, 5040)Processing chunk at (3024, 0)\n",
      "\n",
      "\n",
      "Processing chunk at (3024, 1008)Processing chunk at (3024, 2016)\n",
      "\n",
      "Processing chunk at (3024, 3024)\n",
      "Processing chunk at (3024, 4032)\n",
      "Processing chunk at (3024, 5040)\n",
      "Processing chunk at (4032, 0)\n",
      "Processing chunk at (4032, 1008)\n",
      "Processing chunk at (4032, 2016)Processing chunk at (4032, 3024)\n",
      "Processing chunk at (4032, 4032)\n",
      "\n",
      "Processing chunk at (4032, 5040)Processing chunk at (5040, 0)\n",
      "Processing chunk at (5040, 1008)Processing chunk at (5040, 2016)\n",
      "\n",
      "\n",
      "Processing chunk at (5040, 4032)Processing chunk at (5040, 3024)Processing chunk at (5040, 5040)\n",
      "\n",
      "\n",
      "Processing chunk at (0, 0)Processing chunk at (0, 1008)\n",
      "\n",
      "Processing chunk at (0, 2016)\n",
      "Processing chunk at (0, 3024)Processing chunk at (0, 4032)\n",
      "\n",
      "Processing chunk at (0, 5040)\n",
      "Processing chunk at (1008, 0)Processing chunk at (1008, 1008)Processing chunk at (1008, 2016)\n",
      "\n",
      "Processing chunk at (1008, 3024)\n",
      "\n",
      "Processing chunk at (1008, 4032)\n",
      "Processing chunk at (1008, 5040)\n",
      "Processing chunk at (2016, 0)\n",
      "Processing chunk at (2016, 1008)Processing chunk at (2016, 2016)Processing chunk at (2016, 3024)\n",
      "Processing chunk at (2016, 4032)\n",
      "\n",
      "Processing chunk at (2016, 5040)\n",
      "Processing chunk at (3024, 0)\n",
      "\n",
      "Processing chunk at (3024, 1008)Processing chunk at (3024, 2016)\n",
      "\n",
      "Processing chunk at (3024, 3024)Processing chunk at (3024, 4032)\n",
      "\n",
      "Processing chunk at (3024, 5040)Processing chunk at (4032, 0)\n",
      "Processing chunk at (4032, 1008)\n",
      "\n",
      "Processing chunk at (4032, 2016)Processing chunk at (4032, 3024)\n",
      "Processing chunk at (4032, 4032)\n",
      "\n",
      "Processing chunk at (4032, 5040)Processing chunk at (5040, 0)Processing chunk at (5040, 1008)\n",
      "\n",
      "Processing chunk at (5040, 2016)\n",
      "\n",
      "Processing chunk at (5040, 3024)\n",
      "Processing chunk at (5040, 4032)\n",
      "Processing chunk at (5040, 5040)\n",
      "Processing chunk at (0, 0)\n",
      "Processing chunk at (0, 1008)Processing chunk at (0, 2016)\n",
      "Processing chunk at (0, 3024)\n",
      "\n",
      "Processing chunk at (0, 4032)\n",
      "Processing chunk at (0, 5040)\n",
      "Processing chunk at (1008, 0)\n",
      "Processing chunk at (1008, 1008)\n",
      "Processing chunk at (1008, 2016)\n",
      "Processing chunk at (1008, 3024)\n",
      "Processing chunk at (1008, 4032)\n",
      "Processing chunk at (1008, 5040)Processing chunk at (2016, 0)\n",
      "\n",
      "Processing chunk at (2016, 1008)\n",
      "Processing chunk at (2016, 2016)\n",
      "Processing chunk at (2016, 3024)\n",
      "Processing chunk at (2016, 4032)Processing chunk at (2016, 5040)\n",
      "\n",
      "Processing chunk at (3024, 0)\n",
      "Processing chunk at (3024, 1008)Processing chunk at (3024, 2016)\n",
      "Processing chunk at (3024, 3024)\n",
      "Processing chunk at (3024, 4032)\n",
      "\n",
      "Processing chunk at (3024, 5040)Processing chunk at (4032, 0)\n",
      "\n",
      "Processing chunk at (4032, 1008)Processing chunk at (4032, 2016)\n",
      "Processing chunk at (4032, 3024)\n",
      "Processing chunk at (4032, 4032)\n",
      "\n",
      "Processing chunk at (4032, 5040)Processing chunk at (5040, 0)Processing chunk at (5040, 1008)\n",
      "\n",
      "\n",
      "Processing chunk at (5040, 2016)\n",
      "Processing chunk at (5040, 3024)Processing chunk at (5040, 4032)Processing chunk at (5040, 5040)\n",
      "\n",
      "\n",
      "Processing chunk at (0, 0)\n",
      "Processing chunk at (0, 1008)\n",
      "Processing chunk at (0, 2016)\n",
      "Processing chunk at (0, 3024)Processing chunk at (0, 4032)\n",
      "\n",
      "Processing chunk at (0, 5040)Processing chunk at (1008, 0)\n",
      "\n",
      "Processing chunk at (1008, 1008)\n",
      "Processing chunk at (1008, 2016)\n",
      "Processing chunk at (1008, 3024)\n",
      "Processing chunk at (1008, 4032)\n",
      "Processing chunk at (1008, 5040)\n",
      "Processing chunk at (2016, 0)\n",
      "Processing chunk at (2016, 1008)Processing chunk at (2016, 2016)\n",
      "\n",
      "Processing chunk at (2016, 3024)\n",
      "Processing chunk at (2016, 4032)Processing chunk at (2016, 5040)\n",
      "\n",
      "Processing chunk at (3024, 0)\n",
      "Processing chunk at (3024, 1008)Processing chunk at (3024, 2016)\n",
      "\n",
      "Processing chunk at (3024, 3024)Processing chunk at (3024, 4032)\n",
      "Processing chunk at (3024, 5040)\n",
      "\n",
      "Processing chunk at (4032, 0)\n",
      "Processing chunk at (4032, 1008)\n",
      "Processing chunk at (4032, 2016)Processing chunk at (4032, 3024)\n",
      "Processing chunk at (4032, 4032)Processing chunk at (4032, 5040)\n",
      "Processing chunk at (5040, 0)\n",
      "Processing chunk at (5040, 1008)\n",
      "Processing chunk at (5040, 2016)\n",
      "\n",
      "\n",
      "Processing chunk at (5040, 3024)\n",
      "Processing chunk at (5040, 4032)Processing chunk at (5040, 5040)\n",
      "\n",
      "Processing chunk at (0, 0)Processing chunk at (0, 1008)\n",
      "Processing chunk at (0, 2016)\n",
      "Processing chunk at (0, 3024)\n",
      "\n",
      "Processing chunk at (0, 4032)\n",
      "Processing chunk at (0, 5040)Processing chunk at (1008, 0)\n",
      "\n",
      "Processing chunk at (1008, 1008)\n",
      "Processing chunk at (1008, 2016)\n",
      "Processing chunk at (1008, 3024)\n",
      "Processing chunk at (1008, 4032)Processing chunk at (1008, 5040)\n",
      "Processing chunk at (2016, 0)Processing chunk at (2016, 1008)\n",
      "\n",
      "\n",
      "Processing chunk at (2016, 2016)\n",
      "Processing chunk at (2016, 3024)\n",
      "Processing chunk at (2016, 4032)Processing chunk at (2016, 5040)Processing chunk at (3024, 0)\n",
      "\n",
      "Processing chunk at (3024, 1008)\n",
      "Processing chunk at (3024, 2016)\n",
      "\n",
      "Processing chunk at (3024, 3024)\n",
      "Processing chunk at (3024, 4032)\n",
      "Processing chunk at (3024, 5040)Processing chunk at (4032, 0)Processing chunk at (4032, 1008)Processing chunk at (4032, 2016)\n",
      "Processing chunk at (4032, 3024)\n",
      "Processing chunk at (4032, 4032)\n",
      "\n",
      "\n",
      "\n",
      "Processing chunk at (4032, 5040)\n",
      "Processing chunk at (5040, 0)Processing chunk at (5040, 1008)\n",
      "\n",
      "Processing chunk at (5040, 2016)Processing chunk at (5040, 3024)Processing chunk at (5040, 4032)\n",
      "Processing chunk at (5040, 5040)\n",
      "\n",
      "\n",
      "Processing chunk at (0, 0)Processing chunk at (0, 1008)\n",
      "\n",
      "Processing chunk at (0, 2016)\n",
      "Processing chunk at (0, 3024)\n",
      "Processing chunk at (0, 4032)Processing chunk at (0, 5040)\n",
      "\n",
      "Processing chunk at (1008, 0)Processing chunk at (1008, 1008)\n",
      "Processing chunk at (1008, 2016)\n",
      "\n",
      "Processing chunk at (1008, 3024)\n",
      "Processing chunk at (1008, 4032)\n",
      "Processing chunk at (1008, 5040)Processing chunk at (2016, 0)\n",
      "Processing chunk at (2016, 1008)\n",
      "\n",
      "Processing chunk at (2016, 2016)\n",
      "Processing chunk at (2016, 3024)\n",
      "Processing chunk at (2016, 4032)Processing chunk at (2016, 5040)\n",
      "Processing chunk at (3024, 0)\n",
      "Processing chunk at (3024, 1008)\n",
      "Processing chunk at (3024, 2016)\n",
      "\n",
      "Processing chunk at (3024, 3024)\n",
      "Processing chunk at (3024, 4032)Processing chunk at (3024, 5040)\n",
      "Processing chunk at (4032, 0)Processing chunk at (4032, 1008)\n",
      "\n",
      "\n",
      "Processing chunk at (4032, 2016)Processing chunk at (4032, 3024)\n",
      "\n",
      "Processing chunk at (4032, 4032)\n",
      "Processing chunk at (4032, 5040)\n",
      "Processing chunk at (5040, 0)Processing chunk at (5040, 1008)\n",
      "Processing chunk at (5040, 2016)\n",
      "Processing chunk at (5040, 3024)\n",
      "Processing chunk at (5040, 4032)\n",
      "Processing chunk at (5040, 5040)\n",
      "\n",
      "Processing chunk at (0, 0)\n",
      "Processing chunk at (0, 1008)\n",
      "Processing chunk at (0, 2016)\n",
      "Processing chunk at (0, 3024)\n",
      "Processing chunk at (0, 4032)\n",
      "Processing chunk at (0, 5040)\n",
      "Processing chunk at (1008, 0)\n",
      "Processing chunk at (1008, 1008)\n",
      "Processing chunk at (1008, 2016)Processing chunk at (1008, 3024)\n",
      "Processing chunk at (1008, 4032)\n",
      "Processing chunk at (1008, 5040)\n",
      "\n",
      "Processing chunk at (2016, 0)\n",
      "Processing chunk at (2016, 1008)\n",
      "Processing chunk at (2016, 2016)\n",
      "Processing chunk at (2016, 3024)\n",
      "Processing chunk at (2016, 4032)Processing chunk at (2016, 5040)\n",
      "\n",
      "Processing chunk at (3024, 0)\n",
      "Processing chunk at (3024, 1008)\n",
      "Processing chunk at (3024, 2016)\n",
      "Processing chunk at (3024, 3024)\n",
      "Processing chunk at (3024, 4032)\n",
      "Processing chunk at (3024, 5040)\n",
      "Processing chunk at (4032, 0)Processing chunk at (4032, 1008)\n",
      "Processing chunk at (4032, 2016)Processing chunk at (4032, 3024)\n",
      "\n",
      "\n",
      "Processing chunk at (4032, 4032)\n",
      "Processing chunk at (4032, 5040)\n",
      "Processing chunk at (5040, 0)Processing chunk at (5040, 1008)\n",
      "\n",
      "Processing chunk at (5040, 2016)\n",
      "Processing chunk at (5040, 3024)Processing chunk at (5040, 4032)Processing chunk at (5040, 5040)\n",
      "\n",
      "\n",
      "Processing chunk at (0, 0)Processing chunk at (0, 1008)Processing chunk at (0, 2016)\n",
      "Processing chunk at (0, 3024)\n",
      "\n",
      "\n",
      "Processing chunk at (0, 4032)Processing chunk at (0, 5040)\n",
      "\n",
      "Processing chunk at (1008, 0)\n",
      "Processing chunk at (1008, 1008)\n",
      "Processing chunk at (1008, 2016)\n",
      "Processing chunk at (1008, 3024)\n",
      "Processing chunk at (1008, 4032)\n",
      "Processing chunk at (1008, 5040)\n",
      "Processing chunk at (2016, 0)\n",
      "Processing chunk at (2016, 1008)\n",
      "Processing chunk at (2016, 2016)Processing chunk at (2016, 3024)\n",
      "\n",
      "Processing chunk at (2016, 4032)\n",
      "Processing chunk at (2016, 5040)\n",
      "Processing chunk at (3024, 0)\n",
      "Processing chunk at (3024, 1008)\n",
      "Processing chunk at (3024, 2016)\n",
      "Processing chunk at (3024, 3024)\n",
      "Processing chunk at (3024, 4032)\n",
      "Processing chunk at (3024, 5040)\n",
      "Processing chunk at (4032, 0)\n",
      "Processing chunk at (4032, 1008)\n",
      "Processing chunk at (4032, 2016)Processing chunk at (4032, 3024)\n",
      "\n",
      "Processing chunk at (4032, 4032)\n",
      "Processing chunk at (4032, 5040)\n",
      "Processing chunk at (5040, 0)Processing chunk at (5040, 1008)Processing chunk at (5040, 2016)\n",
      "\n",
      "\n",
      "Processing chunk at (5040, 3024)\n",
      "Processing chunk at (5040, 4032)Processing chunk at (5040, 5040)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk at (0, 0)Processing chunk at (0, 1008)\n",
      "\n",
      "Processing chunk at (0, 2016)\n",
      "Processing chunk at (0, 3024)Processing chunk at (0, 4032)Processing chunk at (0, 5040)\n",
      "\n",
      "Processing chunk at (1008, 0)\n",
      "Processing chunk at (1008, 1008)\n",
      "\n",
      "Processing chunk at (1008, 2016)\n",
      "Processing chunk at (1008, 3024)\n",
      "Processing chunk at (1008, 4032)\n",
      "Processing chunk at (1008, 5040)\n",
      "Processing chunk at (2016, 0)\n",
      "Processing chunk at (2016, 1008)\n",
      "Processing chunk at (2016, 2016)\n",
      "Processing chunk at (2016, 3024)\n",
      "Processing chunk at (2016, 4032)Processing chunk at (2016, 5040)\n",
      "Processing chunk at (3024, 0)\n",
      "Processing chunk at (3024, 1008)\n",
      "\n",
      "Processing chunk at (3024, 2016)Processing chunk at (3024, 3024)\n",
      "\n",
      "Processing chunk at (3024, 4032)\n",
      "Processing chunk at (3024, 5040)Processing chunk at (4032, 0)\n",
      "Processing chunk at (4032, 1008)\n",
      "\n",
      "Processing chunk at (4032, 2016)\n",
      "Processing chunk at (4032, 3024)\n",
      "Processing chunk at (4032, 4032)Processing chunk at (4032, 5040)\n",
      "\n",
      "Processing chunk at (5040, 0)\n",
      "Processing chunk at (5040, 1008)\n",
      "Processing chunk at (5040, 2016)\n",
      "Processing chunk at (5040, 3024)Processing chunk at (5040, 4032)Processing chunk at (5040, 5040)\n",
      "\n",
      "\n",
      "Processing chunk at (0, 0)Processing chunk at (0, 1008)\n",
      "Processing chunk at (0, 2016)\n",
      "Processing chunk at (0, 3024)\n",
      "Processing chunk at (0, 4032)\n",
      "\n",
      "Processing chunk at (0, 5040)\n",
      "Processing chunk at (1008, 0)\n",
      "Processing chunk at (1008, 1008)Processing chunk at (1008, 2016)\n",
      "\n",
      "Processing chunk at (1008, 3024)Processing chunk at (1008, 4032)\n",
      "\n",
      "Processing chunk at (1008, 5040)\n",
      "Processing chunk at (2016, 0)\n",
      "Processing chunk at (2016, 1008)\n",
      "Processing chunk at (2016, 2016)Processing chunk at (2016, 3024)\n",
      "Processing chunk at (2016, 4032)\n",
      "Processing chunk at (2016, 5040)\n",
      "Processing chunk at (3024, 0)\n",
      "Processing chunk at (3024, 1008)Processing chunk at (3024, 2016)\n",
      "Processing chunk at (3024, 3024)\n",
      "\n",
      "\n",
      "Processing chunk at (3024, 4032)\n",
      "Processing chunk at (3024, 5040)Processing chunk at (4032, 0)\n",
      "\n",
      "Processing chunk at (4032, 1008)Processing chunk at (4032, 2016)\n",
      "\n",
      "Processing chunk at (4032, 3024)\n",
      "Processing chunk at (4032, 4032)\n",
      "Processing chunk at (4032, 5040)Processing chunk at (5040, 0)\n",
      "\n",
      "Processing chunk at (5040, 1008)Processing chunk at (5040, 2016)\n",
      "Processing chunk at (5040, 3024)Processing chunk at (5040, 4032)\n",
      "\n",
      "Processing chunk at (5040, 5040)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3b41f6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk at (0, 0)\n",
      "Processing chunk at (0, 1008)Processing chunk at (0, 2016)\n",
      "Processing chunk at (0, 3024)\n",
      "\n",
      "Processing chunk at (0, 4032)Processing chunk at (0, 5040)\n",
      "\n",
      "Processing chunk at (1008, 0)\n",
      "Processing chunk at (1008, 1008)Processing chunk at (1008, 2016)\n",
      "Processing chunk at (1008, 3024)\n",
      "\n",
      "Processing chunk at (1008, 4032)Processing chunk at (1008, 5040)\n",
      "\n",
      "Processing chunk at (2016, 0)Processing chunk at (2016, 1008)Processing chunk at (2016, 2016)\n",
      "\n",
      "\n",
      "Processing chunk at (2016, 3024)\n",
      "Processing chunk at (2016, 4032)\n",
      "Processing chunk at (2016, 5040)\n",
      "Processing chunk at (3024, 0)\n",
      "Processing chunk at (3024, 1008)Processing chunk at (3024, 2016)\n",
      "Processing chunk at (3024, 3024)Processing chunk at (3024, 4032)\n",
      "\n",
      "Processing chunk at (3024, 5040)Processing chunk at (4032, 0)\n",
      "Processing chunk at (4032, 1008)\n",
      "\n",
      "Processing chunk at (4032, 2016)Processing chunk at (4032, 3024)\n",
      "\n",
      "Processing chunk at (4032, 4032)Processing chunk at (4032, 5040)\n",
      "Processing chunk at (5040, 0)\n",
      "\n",
      "\n",
      "Processing chunk at (5040, 1008)\n",
      "Processing chunk at (5040, 2016)\n",
      "Processing chunk at (5040, 3024)Processing chunk at (5040, 4032)\n",
      "Processing chunk at (5040, 5040)\n",
      "\n",
      "Processing chunk at (0, 0)\n",
      "Processing chunk at (0, 1008)\n",
      "Processing chunk at (0, 2016)\n",
      "Processing chunk at (0, 3024)\n",
      "Processing chunk at (0, 4032)\n",
      "Processing chunk at (0, 5040)\n",
      "Processing chunk at (1008, 0)\n",
      "Processing chunk at (1008, 1008)\n",
      "Processing chunk at (1008, 2016)\n",
      "Processing chunk at (1008, 3024)\n",
      "Processing chunk at (1008, 4032)\n",
      "Processing chunk at (1008, 5040)\n",
      "Processing chunk at (2016, 0)\n",
      "Processing chunk at (2016, 1008)Processing chunk at (2016, 2016)\n",
      "\n",
      "Processing chunk at (2016, 3024)\n",
      "Processing chunk at (2016, 4032)\n",
      "Processing chunk at (2016, 5040)\n",
      "Processing chunk at (3024, 0)\n",
      "Processing chunk at (3024, 1008)\n",
      "Processing chunk at (3024, 2016)\n",
      "Processing chunk at (3024, 3024)Processing chunk at (3024, 4032)\n",
      "\n",
      "Processing chunk at (3024, 5040)\n",
      "Processing chunk at (4032, 0)\n",
      "Processing chunk at (4032, 1008)\n",
      "Processing chunk at (4032, 2016)\n",
      "Processing chunk at (4032, 3024)\n",
      "Processing chunk at (4032, 4032)Processing chunk at (4032, 5040)\n",
      "Processing chunk at (5040, 0)\n",
      "\n",
      "Processing chunk at (5040, 1008)\n",
      "Processing chunk at (5040, 2016)Processing chunk at (5040, 3024)\n",
      "\n",
      "Processing chunk at (5040, 4032)\n",
      "Processing chunk at (5040, 5040)\n",
      "Processing chunk at (0, 0)\n",
      "Processing chunk at (0, 1008)\n",
      "Processing chunk at (0, 2016)\n",
      "Processing chunk at (0, 3024)\n",
      "Processing chunk at (0, 4032)\n",
      "Processing chunk at (0, 5040)\n",
      "Processing chunk at (1008, 0)Processing chunk at (1008, 1008)\n",
      "Processing chunk at (1008, 2016)\n",
      "\n",
      "Processing chunk at (1008, 3024)\n",
      "Processing chunk at (1008, 4032)Processing chunk at (1008, 5040)\n",
      "Processing chunk at (2016, 0)\n",
      "\n",
      "Processing chunk at (2016, 1008)\n",
      "Processing chunk at (2016, 2016)Processing chunk at (2016, 3024)\n",
      "Processing chunk at (2016, 4032)\n",
      "\n",
      "Processing chunk at (2016, 5040)\n",
      "Processing chunk at (3024, 0)\n",
      "Processing chunk at (3024, 1008)\n",
      "Processing chunk at (3024, 2016)\n",
      "Processing chunk at (3024, 3024)\n",
      "Processing chunk at (3024, 4032)Processing chunk at (3024, 5040)\n",
      "\n",
      "Processing chunk at (4032, 0)\n",
      "Processing chunk at (4032, 1008)\n",
      "Processing chunk at (4032, 2016)Processing chunk at (4032, 3024)\n",
      "Processing chunk at (4032, 4032)\n",
      "\n",
      "Processing chunk at (4032, 5040)Processing chunk at (5040, 0)Processing chunk at (5040, 1008)\n",
      "\n",
      "Processing chunk at (5040, 2016)\n",
      "Processing chunk at (5040, 3024)\n",
      "Processing chunk at (5040, 4032)\n",
      "Processing chunk at (5040, 5040)\n",
      "\n",
      "Processing chunk at (0, 0)\n",
      "Processing chunk at (0, 1008)\n",
      "Processing chunk at (0, 2016)\n",
      "Processing chunk at (0, 3024)Processing chunk at (0, 4032)\n",
      "Processing chunk at (0, 5040)\n",
      "Processing chunk at (1008, 0)\n",
      "\n",
      "Processing chunk at (1008, 1008)\n",
      "Processing chunk at (1008, 2016)\n",
      "Processing chunk at (1008, 3024)\n",
      "Processing chunk at (1008, 4032)\n",
      "Processing chunk at (1008, 5040)\n",
      "Processing chunk at (2016, 0)\n",
      "Processing chunk at (2016, 1008)Processing chunk at (2016, 2016)\n",
      "Processing chunk at (2016, 3024)\n",
      "\n",
      "Processing chunk at (2016, 4032)\n",
      "Processing chunk at (2016, 5040)Processing chunk at (3024, 0)\n",
      "Processing chunk at (3024, 1008)\n",
      "\n",
      "Processing chunk at (3024, 2016)\n",
      "Processing chunk at (3024, 3024)\n",
      "Processing chunk at (3024, 4032)\n",
      "Processing chunk at (3024, 5040)\n",
      "Processing chunk at (4032, 0)Processing chunk at (4032, 1008)Processing chunk at (4032, 2016)\n",
      "\n",
      "Processing chunk at (4032, 3024)\n",
      "\n",
      "Processing chunk at (4032, 4032)\n",
      "Processing chunk at (4032, 5040)Processing chunk at (5040, 0)\n",
      "\n",
      "Processing chunk at (5040, 1008)Processing chunk at (5040, 2016)\n",
      "Processing chunk at (5040, 3024)\n",
      "Processing chunk at (5040, 4032)\n",
      "\n",
      "Processing chunk at (5040, 5040)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Image layer 'macrophage' at 0x7f0fb3368eb0>,\n",
       " <Image layer 'mtb' at 0x7f0fc41ba130>]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = napari.Viewer()\n",
    "v.add_image(image,\n",
    "            channel_axis=1,\n",
    "            name=[\"macrophage\", \"mtb\"],\n",
    "            colormap=[\"green\", \"magenta\"],\n",
    "            )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "4bbbcd4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 34.88 MiB </td> <td> 3.88 MiB </td></tr>\n",
       "    <tr><th> Shape </th><td> (6048, 6048) </td> <td> (2016, 2016) </td></tr>\n",
       "    <tr><th> Count </th><td> 19 Tasks </td><td> 9 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> uint8 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"170\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"120\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"40\" x2=\"120\" y2=\"40\" />\n",
       "  <line x1=\"0\" y1=\"80\" x2=\"120\" y2=\"80\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"120\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"40\" y1=\"0\" x2=\"40\" y2=\"120\" />\n",
       "  <line x1=\"80\" y1=\"0\" x2=\"80\" y2=\"120\" />\n",
       "  <line x1=\"120\" y1=\"0\" x2=\"120\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 120.0,0.0 120.0,120.0 0.0,120.0\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"60.000000\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >6048</text>\n",
       "  <text x=\"140.000000\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,140.000000,60.000000)\">6048</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<fuse_func, shape=(6048, 6048), dtype=uint8, chunksize=(2016, 2016), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_array"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aero",
   "language": "python",
   "name": "aero"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
